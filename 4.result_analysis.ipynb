{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f87bc54c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6654 & 5582 & 5028 & 5014 & 3042 & 1999\n",
      "500 & 500 & 495 & 500 & 485 & 470\n",
      "13.308 & 11.164 & 10.1576 & 10.028 & 6.2722 & 4.2532\n",
      "0.8335 & 0.6992 & 0.6298 & 0.6281 & 0.3811 & 0.2504\n"
     ]
    }
   ],
   "source": [
    "# Vanilla testing\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "all_lms = [\"qwen2_0B5\", \"qwen2_1B5\", \"qwen2_7B\", \"mistral03_7B\", \"llama3_8B\", \"gemma2_9B\"]\n",
    "result_403_path = \"result/MTMT/{}/MR0_round_original.pickle\"\n",
    "result_s97_path = \"result/MTMT/{}/S97_MR0_SNP_s06_original.pickle\"\n",
    "\n",
    "CHECK_LLM = 0\n",
    "\n",
    "row_bugs = []\n",
    "row_ETC = []\n",
    "row_BTC = []\n",
    "row_BSV = []\n",
    "row_PRate = []\n",
    "\n",
    "for chosen_lm in all_lms:\n",
    "    df_403 = pd.read_pickle(result_403_path.format(chosen_lm))\n",
    "    df_s97 = pd.read_pickle(result_s97_path.format(chosen_lm))\n",
    "\n",
    "    df_full = pd.concat([df_403, df_s97], axis=0, ignore_index=True)\n",
    "\n",
    "    df_full[\"score_semantic\"]=[item[0] if item else None for item in df_full[\"score_semantic\"]]\n",
    "    Bugs = len(df_full[df_full[\"score_semantic\"]<0.6])\n",
    "    ETC = len(df_full[df_full[\"score_semantic\"]<0.6][\"source_uuid\"].unique())\n",
    "    BTC = Bugs/ETC\n",
    "    PRate = len(df_full[df_full[\"score_semantic\"]<0.6]) / 7983\n",
    "\n",
    "    row_bugs.append(Bugs)\n",
    "    row_ETC.append(ETC)\n",
    "    row_BTC.append(BTC)\n",
    "    row_PRate.append(PRate)\n",
    "\n",
    "for row in [row_bugs, row_ETC, row_BTC, row_PRate]:\n",
    "    row_text = \" & \".join([str(round(item,4)) for item in row])\n",
    "    print(row_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24df1628",
   "metadata": {},
   "source": [
    "# RQ1: Effectiveness"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43fe5e80",
   "metadata": {},
   "source": [
    "### MORTAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1e85bc57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MR1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6375/6375 [00:16<00:00, 380.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MR2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 151/151 [00:03<00:00, 37.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MR3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6194/6194 [01:56<00:00, 53.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MR4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 151/151 [00:05<00:00, 25.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DS1 & 31942  & 100.0\\%  & 79.261  & 0.375  & 86.5\\% & 29.688 \\\\ \n",
      "MR1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6375/6375 [00:16<00:00, 389.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MR2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 151/151 [00:03<00:00, 41.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MR3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6194/6194 [02:08<00:00, 48.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MR4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 151/151 [00:06<00:00, 23.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DS2 & 27799  & 100.0\\%  & 68.98  & 0.368  & 75.3\\% & 25.378 \\\\ \n",
      "MR1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6375/6375 [00:16<00:00, 393.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MR2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 151/151 [00:03<00:00, 42.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MR3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6194/6194 [04:38<00:00, 22.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MR4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 151/151 [00:06<00:00, 24.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DS3 & 24634  & 100.0\\%  & 61.127  & 0.35  & 66.7\\% & 21.381 \\\\ \n",
      "MR1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6375/6375 [00:16<00:00, 388.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MR2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 151/151 [00:03<00:00, 40.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MR3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6194/6194 [05:40<00:00, 18.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MR4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 151/151 [00:05<00:00, 28.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DS4 & 22919  & 100.0\\%  & 56.871  & 0.344  & 62.1\\% & 19.551 \\\\ \n",
      "MR1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6375/6375 [00:16<00:00, 390.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MR2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 151/151 [00:03<00:00, 42.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MR3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6194/6194 [05:01<00:00, 20.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MR4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 151/151 [00:05<00:00, 26.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DS5 & 17264  & 100.0\\%  & 42.839  & 0.346  & 46.8\\% & 14.831 \\\\ \n",
      "MR1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6375/6375 [00:16<00:00, 393.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MR2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 151/151 [00:03<00:00, 42.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MR3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6194/6194 [06:37<00:00, 15.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MR4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 151/151 [00:05<00:00, 27.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DS6 & 12805  & 99.8\\%  & 31.853  & 0.348  & 34.7\\% & 11.067 \\\\ \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "import json \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from itertools import combinations, product\n",
    "from tqdm import tqdm\n",
    "from multi_turn_test import MultiTurnConversation as MTC\n",
    "score_tools = MTC.score_utils()\n",
    "\n",
    "MORTAR_Perts = [\n",
    "    \"P1_round_shuffle\",\n",
    "    \"P2_round_reduce\",\n",
    "    \"P3_round_duplicate\",\n",
    "    \"P4_round_reduce_shuffle\",\n",
    "    \"P5_round_shuffle_duplicate\",\n",
    "    ]\n",
    "\n",
    "dataset_jsons = {}\n",
    "for this_pert in MORTAR_Perts:\n",
    "    with open(f\"data/{this_pert}.json\", \"r\") as f:\n",
    "        dataset_jsons[this_pert] = json.load(f)\n",
    "\n",
    "mapping_original_round_appear_in_pert = {} # Oid-r1 -> [P1-r1, P2-r3 ...]\n",
    "all_original_round_ids = []\n",
    "\n",
    "dialogue_round_keys_403 = json.load(open(\"data/dialogue_round_keys_403.json\", \"r\"))\n",
    "for original_dialogue_key in dialogue_round_keys_403.keys():\n",
    "    for original_round_key in dialogue_round_keys_403[original_dialogue_key]:\n",
    "        original_round_uid = original_dialogue_key + \"##\" + original_round_key\n",
    "        all_original_round_ids.append(original_round_uid)\n",
    "        mapping_original_round_appear_in_pert[original_round_uid] = []\n",
    "\n",
    "for pert_type in MORTAR_Perts:\n",
    "    this_pert_dataset = dataset_jsons[pert_type]\n",
    "    for pert_dialogue_key in this_pert_dataset.keys():\n",
    "        for pert_round_key in this_pert_dataset[pert_dialogue_key].keys():\n",
    "            pert_round_uid = pert_type + \"#pert#\" +pert_dialogue_key + \"#pert#\" + pert_round_key\n",
    "            original_round = this_pert_dataset[pert_dialogue_key][pert_round_key]['is_original_round']\n",
    "            original_round_uid = pert_dialogue_key + \"##\" + original_round # key in mapping_original_round_appear_in_pert\n",
    "            mapping_original_round_appear_in_pert[original_round_uid].append(pert_round_uid)\n",
    "\n",
    "\n",
    "list_pert_dialogue_usage_mr1 = {} # O1-r3 : [P1-O1-r2, P2...], (pert_round_uid), expect output(P1-O1-r2) == answer(O1-r3)\n",
    "list_pert_dialogue_usage_mr2 = {} # O1-r2 : [P2-O1-r1, ...], (pert_round_uid), expect output(P2-O1-r1) != answer(O1-r2)\n",
    "\n",
    "dict_pert_dialogue_usage_mr3 = {} # O1-r1 : {1:[P1-O1-r1, P2-O1-r3, ...]}, expect output(P1-O1-r1) == output(P2-O1-r3) and ...\n",
    "dict_pert_dialogue_usage_mr4 = {} # O1-r1 : {0:[P2-O1-r1] 1:[P1-O1-r1, P2-O1-r3]}, expect output(P2-O1-r1) != output(P1-O1-r1) and output(P2-O1-r1) != output(P2-O1-r3)\n",
    "\n",
    "for original_round_id in all_original_round_ids:\n",
    "    list_pert_dialogue_usage_mr1[original_round_id] = []\n",
    "    list_pert_dialogue_usage_mr2[original_round_id] = []\n",
    "\n",
    "    this_ori_dialogue_appear_in_pert = mapping_original_round_appear_in_pert[original_round_id]\n",
    "\n",
    "    pert_answerable_q_sum = 0\n",
    "    dict_pert_answerability = {}\n",
    "\n",
    "    for pert_round_id in this_ori_dialogue_appear_in_pert:\n",
    "        pert_type = pert_round_id.split(\"#pert#\")[0]\n",
    "        pert_dialogue_key = pert_round_id.split(\"#pert#\")[1]\n",
    "        pert_round_key = pert_round_id.split(\"#pert#\")[2]\n",
    "\n",
    "        answerability_this_pert_round = (dataset_jsons[pert_type][pert_dialogue_key][pert_round_key][\"Answer\"] != \"unknown\") # changed answerability\n",
    "        dict_pert_answerability[pert_round_key] = answerability_this_pert_round\n",
    "        pert_answerable_q_sum += answerability_this_pert_round\n",
    "\n",
    "        if answerability_this_pert_round:\n",
    "            list_pert_dialogue_usage_mr1[original_round_id].append(pert_round_id)\n",
    "        else:\n",
    "            if dataset_jsons[pert_type][pert_dialogue_key][pert_round_key][\"FLAG_answer_changed\"]:\n",
    "                list_pert_dialogue_usage_mr2[original_round_id].append(pert_round_id)\n",
    "\n",
    "    if pert_answerable_q_sum == len(this_ori_dialogue_appear_in_pert) and pert_answerable_q_sum > 0: # all pert rounds are answerable\n",
    "        dict_pert_dialogue_usage_mr3[original_round_id] = {1:[]}\n",
    "        dict_pert_dialogue_usage_mr3[original_round_id][1] = copy.deepcopy(list_pert_dialogue_usage_mr1[original_round_id])\n",
    "    elif 0<pert_answerable_q_sum<len(this_ori_dialogue_appear_in_pert): # some pert rounds are answerable, some are not\n",
    "        dict_pert_dialogue_usage_mr4[original_round_id] = {0:[],1:[]}\n",
    "        dict_pert_dialogue_usage_mr4[original_round_id][0] = copy.deepcopy(list_pert_dialogue_usage_mr2[original_round_id])\n",
    "        dict_pert_dialogue_usage_mr4[original_round_id][1] = copy.deepcopy(list_pert_dialogue_usage_mr1[original_round_id])\n",
    "\n",
    "for original_round_id in list_pert_dialogue_usage_mr1.copy().keys():\n",
    "    if len(list_pert_dialogue_usage_mr1[original_round_id]) == 0:\n",
    "        del list_pert_dialogue_usage_mr1[original_round_id]\n",
    "\n",
    "for original_round_id in list_pert_dialogue_usage_mr2.copy().keys():\n",
    "    if len(list_pert_dialogue_usage_mr2[original_round_id]) == 0:\n",
    "        del list_pert_dialogue_usage_mr2[original_round_id]\n",
    "\n",
    "for original_round_id in dict_pert_dialogue_usage_mr3.copy().keys():\n",
    "    if len(dict_pert_dialogue_usage_mr3[original_round_id][1]) < 2:\n",
    "        del dict_pert_dialogue_usage_mr3[original_round_id]\n",
    "\n",
    "for original_round_id in dict_pert_dialogue_usage_mr4.copy().keys():\n",
    "    if len(dict_pert_dialogue_usage_mr4[original_round_id][0]) == 0 or len(dict_pert_dialogue_usage_mr4[original_round_id][1]) == 0:\n",
    "        del dict_pert_dialogue_usage_mr4[original_round_id]\n",
    "\n",
    "all_lms = [\"qwen2_0B5\", \"qwen2_1B5\", \"qwen2_7B\", \"mistral03_7B\", \"llama3_8B\", \"gemma2_9B\"]\n",
    "\n",
    "dist_LLM_DS_name = {\"qwen2_0B5\":\"DS1\", \"qwen2_1B5\":\"DS2\", \"qwen2_7B\":\"DS3\", \"mistral03_7B\":\"DS4\", \"llama3_8B\":\"DS5\", \"gemma2_9B\":\"DS6\"}\n",
    "\n",
    "MORTAR_Perts = [\"P1_round_shuffle\", \"P2_round_reduce\", \"P3_round_duplicate\", \"P4_round_reduce_shuffle\", \"P5_round_shuffle_duplicate\"]\n",
    "result_path = \"result/MTMT/{}/{}.pickle\"\n",
    "\n",
    "dataset_original = json.load(open(\"data/500_MR0_SNP_s06_original.json\", \"r\"))\n",
    "\n",
    "MORTAR_result_DataFrames = {}\n",
    "for chosen_lm in all_lms:\n",
    "    for chosen_pert in MORTAR_Perts:\n",
    "        df_result = pd.read_pickle(result_path.format(chosen_lm, chosen_pert))\n",
    "        df_result = df_result[df_result[\"role\"] == \"assistant\"]\n",
    "        df_result[\"score_semantic\"]=[item[0] if item else None for item in df_result[\"score_semantic\"]]\n",
    "        df_result[\"row_uid\"] = [ (df_result.loc[row, \"source_uuid\"] + \"##\" + str(df_result.loc[row,\"round\"])) for row in df_result.index]\n",
    "        df_result[\"id_pert_round\"] = [chosen_pert + \"#pert#\" + df_result.loc[row, \"source_uuid\"] + \"#pert#\" + \"Round \" + str(df_result.loc[row,\"round\"]) for row in df_result.index]\n",
    "        MORTAR_result_DataFrames[chosen_lm + \"_\" + chosen_pert] = copy.deepcopy(df_result)\n",
    "\n",
    "def get_original_answer(id_pert_round):\n",
    "    pert_dialogue_key = id_pert_round.split(\"#pert#\")[1]\n",
    "    original_round_key = id_pert_round.split(\"#pert#\")[2]\n",
    "    return dataset_original[pert_dialogue_key][original_round_key][\"Answer\"]\n",
    "\n",
    "def get_pert_round_answer(id_pert_round):\n",
    "    pert_type = id_pert_round.split(\"#pert#\")[0]\n",
    "    df_result = MORTAR_result_DataFrames[chosen_lm + \"_\" + pert_type]\n",
    "    return df_result[df_result[\"id_pert_round\"] == id_pert_round][\"content\"].values[0]\n",
    "\n",
    "def get_pert_round_expected_answer(id_pert_round):\n",
    "    pert_type = id_pert_round.split(\"#pert#\")[0]\n",
    "    df_result = MORTAR_result_DataFrames[chosen_lm + \"_\" + pert_type]\n",
    "    return df_result[df_result[\"id_pert_round\"] == id_pert_round][\"expected_answer\"].values[0][0]\n",
    "\n",
    "def MORTAR_MR1_conflict_detector(id_pert_round):\n",
    "    pert_type = id_pert_round.split(\"#pert#\")[0]\n",
    "    df_result = MORTAR_result_DataFrames[chosen_lm + \"_\" + pert_type]\n",
    "    score = df_result[df_result[\"id_pert_round\"] == id_pert_round][\"score_semantic\"].values[0]\n",
    "\n",
    "    BUG_FLAG=False\n",
    "    if score < 0.6:\n",
    "        BUG_FLAG=True\n",
    "    ERROR = abs((1 - score)/2)\n",
    "    return BUG_FLAG, ERROR\n",
    "\n",
    "def MORTAR_MR2_conflict_detector(id_pert_round):\n",
    "    output = get_pert_round_answer(id_pert_round)\n",
    "    answer_original = get_original_answer(id_pert_round)\n",
    "    score = score_tools.semantic_similarity(output, answer_original)\n",
    "\n",
    "    BUG_FLAG=False\n",
    "    if score > 0.6:\n",
    "        BUG_FLAG=True\n",
    "\n",
    "    similarity_to_unknown = score_tools.semantic_similarity(output, \"unknown\")\n",
    "    ERROR = abs((1 - similarity_to_unknown)/2)\n",
    "    return BUG_FLAG, ERROR\n",
    "\n",
    "def MORTAR_MR3_conflict_detector(list_pert_round_1):\n",
    "    two_element_combinations = list(combinations(list_pert_round_1, 2))\n",
    "    max_similarity = -1\n",
    "    for pair in two_element_combinations:\n",
    "        sentence_1 = get_pert_round_answer(pair[0])\n",
    "        sentence_2 = get_pert_round_answer(pair[1])\n",
    "        similarity = score_tools.semantic_similarity(sentence_1, sentence_2)\n",
    "\n",
    "        if similarity > max_similarity:\n",
    "            max_similarity = similarity\n",
    "\n",
    "        if similarity < 0.6:\n",
    "            ERROR = abs((1 - similarity)/2)\n",
    "            return True, ERROR, pair\n",
    "    return False, abs((1 - max_similarity)/2), None\n",
    "\n",
    "def MORTAR_MR4_conflict_detector(list_pert_rounds_1, list_pert_rounds_0):\n",
    "    max_similarity = -1\n",
    "    pairs = product(list_pert_rounds_1, list_pert_rounds_0)\n",
    "    for pair in pairs:\n",
    "        id_pert_round_1 = pair[0]\n",
    "        id_pert_round_0 = pair[1]\n",
    "        output_1 = get_pert_round_answer(id_pert_round_1)\n",
    "        output_0 = get_pert_round_answer(id_pert_round_0)\n",
    "\n",
    "        similarity = score_tools.semantic_similarity(output_1, output_0)\n",
    "        \n",
    "        if similarity > max_similarity:\n",
    "            max_similarity = similarity\n",
    "\n",
    "        if similarity > 0.6:\n",
    "            answer_1 = get_pert_round_expected_answer(id_pert_round_1)\n",
    "            answer_0 = get_pert_round_expected_answer(id_pert_round_0)\n",
    "            \n",
    "            embed_1 = score_tools.encoder(output_1)-score_tools.encoder(output_0)\n",
    "            embed_2 = score_tools.encoder(answer_1)-score_tools.encoder(answer_0)\n",
    "            cos = score_tools.cosine_similarity(embed_1, embed_2)\n",
    "            ERROR = abs((1 - cos)/2)\n",
    "            return True, ERROR, pair\n",
    "    \n",
    "    return False, abs((-1 - max_similarity)/2), None\n",
    "\n",
    "\n",
    "PAPER_RESULT_MORTAR = {}\n",
    "FOUNDED_Bugs_MORTAR = {}\n",
    "\n",
    "for chosen_lm in all_lms:\n",
    "\n",
    "    MR1_TOOGLE = True\n",
    "    MR2_TOOGLE = True\n",
    "    MR3_TOOGLE = True\n",
    "    MR4_TOOGLE = True\n",
    "\n",
    "    MORTAR_Bugs = 0\n",
    "    MORTAR_ETC = []\n",
    "    MORTAR_BTC = 0\n",
    "    MORTAR_BSV_sum = 0\n",
    "    MORTAR_Amount_detection = 0\n",
    "\n",
    "    # MR1\n",
    "    MR1_Amount_detection = 0\n",
    "    MR1_Bugs = []\n",
    "    MR1_ETC = []\n",
    "    MR1_BSV_sum = 0\n",
    "\n",
    "    if MR1_TOOGLE:\n",
    "        print(\"MR1\")\n",
    "        for original_round_id in tqdm(list_pert_dialogue_usage_mr1.keys()):\n",
    "            original_dialogue_key = original_round_id.split(\"##\")[0]\n",
    "\n",
    "            for pert_round_id in list_pert_dialogue_usage_mr1[original_round_id]:\n",
    "                pert_type = pert_round_id.split(\"#pert#\")[0]\n",
    "                pert_dialogue_key = pert_round_id.split(\"#pert#\")[1]\n",
    "\n",
    "                BUG_FLAG, ERROR = MORTAR_MR1_conflict_detector(pert_round_id)\n",
    "                MR1_Amount_detection += 1\n",
    "                if BUG_FLAG:\n",
    "                    id_bug = \"MR1#mr#\" + original_round_id + \"#bug#\" + pert_round_id\n",
    "                    MR1_Bugs.append(id_bug)\n",
    "                    MR1_ETC.append(original_dialogue_key)\n",
    "                    MR1_BSV_sum += ERROR\n",
    "\n",
    "    # MR2\n",
    "    MR2_Amount_detection = 0\n",
    "    MR2_Bugs = []\n",
    "    MR2_ETC = []\n",
    "    MR2_BSV_sum = 0\n",
    "\n",
    "    if MR2_TOOGLE:\n",
    "        print(\"MR2\")\n",
    "        for original_round_id in tqdm(list_pert_dialogue_usage_mr2.keys()):\n",
    "            original_dialogue_key = original_round_id.split(\"##\")[0]\n",
    "\n",
    "            for pert_round_id in list_pert_dialogue_usage_mr2[original_round_id]:\n",
    "                pert_type = pert_round_id.split(\"#pert#\")[0]\n",
    "                pert_dialogue_key = pert_round_id.split(\"#pert#\")[1]\n",
    "                # pert_round_key = pert_round_id.split(\"#pert#\")[2]\n",
    "\n",
    "                # id_test_case = pert_type + \"#testcase#\"\n",
    "                BUG_FLAG, ERROR = MORTAR_MR2_conflict_detector(pert_round_id)\n",
    "                MR2_Amount_detection += 1\n",
    "                if BUG_FLAG:\n",
    "                    id_bug = \"MR2#mr#\" + original_round_id + \"#bug#\" + pert_round_id\n",
    "                    MR2_Bugs.append(id_bug)\n",
    "                    MR2_ETC.append(original_dialogue_key)\n",
    "                    MR2_BSV_sum += ERROR\n",
    "\n",
    "    # MR3\n",
    "    MR3_Amount_detection = 0\n",
    "    MR3_Bugs = []\n",
    "    MR3_ETC = []\n",
    "    MR3_BSV_sum = 0\n",
    "    if MR3_TOOGLE:\n",
    "        print(\"MR3\")\n",
    "        for original_round_id in tqdm(dict_pert_dialogue_usage_mr3.keys()):\n",
    "            original_dialogue_key = original_round_id.split(\"##\")[0]\n",
    "\n",
    "            # id_test_case = pert_type + \"#testcase#\"\n",
    "            BUG_FLAG, ERROR, Bug_pair = MORTAR_MR3_conflict_detector(dict_pert_dialogue_usage_mr3[original_round_id][1])\n",
    "            MR3_Amount_detection += 1\n",
    "            if BUG_FLAG:\n",
    "                id_bug =  \"MR3#mr#\" + original_round_id + \"#bug#\" + Bug_pair[0] + \"#bug#\" + Bug_pair[1]\n",
    "                MR3_Bugs.append(id_bug)\n",
    "                MR3_ETC.append(original_dialogue_key)\n",
    "                MR3_BSV_sum += ERROR\n",
    "\n",
    "    # MR4\n",
    "    MR4_Amount_detection = 0\n",
    "    MR4_Bugs = []\n",
    "    MR4_ETC = []\n",
    "    MR4_BSV_sum = 0\n",
    "    if MR4_TOOGLE:\n",
    "        print(\"MR4\")\n",
    "        for original_round_id in tqdm(dict_pert_dialogue_usage_mr4.keys()):\n",
    "            original_dialogue_key = original_round_id.split(\"##\")[0]\n",
    "\n",
    "            list_pert_round_id_1 = dict_pert_dialogue_usage_mr4[original_round_id][1]\n",
    "            list_pert_round_id_0 = dict_pert_dialogue_usage_mr4[original_round_id][0]\n",
    "\n",
    "            BUG_FLAG, ERROR, Bug_pair = MORTAR_MR4_conflict_detector(dict_pert_dialogue_usage_mr4[original_round_id][1], dict_pert_dialogue_usage_mr4[original_round_id][0])\n",
    "            MR4_Amount_detection += 1\n",
    "            if BUG_FLAG:\n",
    "                id_bug = \"MR4#mr#\" + original_round_id + \"#bug#\" + Bug_pair[0] + \"#bug#\" + Bug_pair[1]\n",
    "                MR4_Bugs.append(id_bug)\n",
    "                MR4_ETC.append(original_dialogue_key)\n",
    "                MR4_BSV_sum += ERROR\n",
    "\n",
    "    MORTAR_Bug_founded = MR1_Bugs + MR2_Bugs + MR3_Bugs + MR4_Bugs\n",
    "    MORTAR_Bugs = len(MORTAR_Bug_founded)\n",
    "    MORTAR_ETC_ids = set(MR1_ETC + MR2_ETC + MR3_ETC + MR4_ETC)\n",
    "    MORTAR_ETC = round(len(MORTAR_ETC_ids)/403*100, 1) # percentage\n",
    "    MORTAR_BTC = MORTAR_Bugs/len(MORTAR_ETC_ids)\n",
    "    MORTAR_BTS = MORTAR_Bugs/403\n",
    "    MORTAR_BSV = (MR1_BSV_sum + MR2_BSV_sum + MR3_BSV_sum + MR4_BSV_sum)/MORTAR_Bugs\n",
    "    MORTAR_Amount_detection = MR1_Amount_detection + MR2_Amount_detection + MR3_Amount_detection + MR4_Amount_detection\n",
    "    MORTAR_Rate_Plus = round(MORTAR_Bugs/MORTAR_Amount_detection*100, 1) \n",
    "    MORTAR_BugScore = MORTAR_BSV * MORTAR_BTS\n",
    "\n",
    "    PAPER_RESULT_MORTAR[chosen_lm] = {\"Bugs\":MORTAR_Bugs, \"ETC\":MORTAR_ETC, \"BTC\":round(MORTAR_BTC,3), \"BSV\":round(MORTAR_BSV,3), \"Rate+\":MORTAR_Rate_Plus, \"BugScore\":round(MORTAR_BugScore,3)}\n",
    "    FOUNDED_Bugs_MORTAR[chosen_lm] = MORTAR_Bug_founded\n",
    "\n",
    "    print(\"{} & {}  & {}\\\\%  & {}  & {}  & {}\\\\% & {} \\\\\\\\ \".format(dist_LLM_DS_name[chosen_lm], MORTAR_Bugs, MORTAR_ETC, round(MORTAR_BTC,3), round(MORTAR_BSV,3), MORTAR_Rate_Plus, round(MORTAR_BugScore,3)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af6757f8",
   "metadata": {},
   "source": [
    "### METAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d3a51e21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MR1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6203/6203 [00:09<00:00, 686.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DS1 & 12652  & 100.0\\%  & 25.304  & 0.388  & 86.3\\% & 9.823 \\\\ \n",
      "MR1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6203/6203 [00:09<00:00, 680.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DS2 & 10710  & 100.0\\%  & 21.42  & 0.37  & 73.0\\% & 7.935 \\\\ \n",
      "MR1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6203/6203 [00:09<00:00, 677.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DS3 & 9754  & 100.0\\%  & 19.508  & 0.354  & 66.5\\% & 6.9 \\\\ \n",
      "MR1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6203/6203 [00:09<00:00, 676.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DS4 & 9876  & 100.0\\%  & 19.752  & 0.357  & 67.3\\% & 7.043 \\\\ \n",
      "MR1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6203/6203 [00:09<00:00, 683.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DS5 & 6631  & 99.8\\%  & 13.289  & 0.356  & 45.2\\% & 4.718 \\\\ \n",
      "MR1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6203/6203 [00:08<00:00, 696.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DS6 & 4199  & 98.2\\%  & 8.552  & 0.346  & 28.6\\% & 2.908 \\\\ \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "import json \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from itertools import combinations, product\n",
    "from tqdm import tqdm\n",
    "from multi_turn_test import MultiTurnConversation as MTC\n",
    "score_tools = MTC.score_utils()\n",
    "\n",
    "METAL_perts = [\n",
    "    \"500_MR10_SNP_s06_synonym_replacement\",\n",
    "    \"500_MR13_SNP_s06_add_words\",\n",
    "    \"500_MR11_SNP_s06_introduce_typos\",\n",
    "    \"500_MR12_SNP_s06_to_leet\",\n",
    "]\n",
    "\n",
    "dataset_jsons = {}\n",
    "\n",
    "for this_pert in METAL_perts:\n",
    "    with open(f\"data/{this_pert}.json\", \"r\") as f:\n",
    "        dataset_jsons[this_pert] = json.load(f)\n",
    "\n",
    "for this_pert in METAL_perts:\n",
    "    id_pert_dialogues = []\n",
    "    id_pert_dialogue_has_uanswerability_round = []\n",
    "    id_pert_dialogue_usage_mr1 = []\n",
    "    id_pert_dialogue_usage_mr2 = []\n",
    "\n",
    "    id_pert_rounds = []\n",
    "    id_pert_rounds_un_answerable = []\n",
    "    id_pert_rounds_question_changed = []\n",
    "    \n",
    "    this_pert_dataset = dataset_jsons[this_pert]\n",
    "    for dialogue_key in this_pert_dataset.keys():\n",
    "        id_pert_dialogues.append(dialogue_key)\n",
    "        for pert_round_key in this_pert_dataset[dialogue_key].keys():\n",
    "            pert_round_uid = dialogue_key + \"##\" + pert_round_key\n",
    "            id_pert_rounds.append(pert_round_uid)\n",
    "\n",
    "            if this_pert_dataset[dialogue_key][pert_round_key][\"Question\"] != this_pert_dataset[dialogue_key][pert_round_key][\"Original_question\"]:\n",
    "                id_pert_rounds_question_changed.append(pert_round_uid)\n",
    "                id_pert_dialogue_usage_mr1.append(dialogue_key) if dialogue_key not in id_pert_dialogue_usage_mr1 else None\n",
    "\n",
    "mapping_original_round_appear_in_pert = {} # Oid-r1 -> [P1-r1, P2-r3 ...]\n",
    "all_original_round_ids = []\n",
    "all_pert_round_ids = []\n",
    "\n",
    "dialogue_round_keys_500 = json.load(open(\"data/dialogue_round_keys_500.json\", \"r\"))\n",
    "for original_dialogue_key in dialogue_round_keys_500.keys():\n",
    "    for original_round_key in dialogue_round_keys_500[original_dialogue_key]:\n",
    "        original_round_uid = original_dialogue_key + \"##\" + original_round_key\n",
    "        all_original_round_ids.append(original_round_uid)\n",
    "        mapping_original_round_appear_in_pert[original_round_uid] = []\n",
    "\n",
    "for pert_type in METAL_perts:\n",
    "    this_pert_dataset = dataset_jsons[pert_type]\n",
    "    for pert_dialogue_key in this_pert_dataset.keys():\n",
    "        for pert_round_key in this_pert_dataset[pert_dialogue_key].keys():\n",
    "            id_pert_round = pert_type + \"#pert#\" +pert_dialogue_key + \"#pert#\" + pert_round_key\n",
    "            all_pert_round_ids.append(id_pert_round)\n",
    "            original_round = this_pert_dataset[pert_dialogue_key][pert_round_key]['is_original_round']\n",
    "            original_round_uid = pert_dialogue_key + \"##\" + original_round # key in mapping_original_round_appear_in_pert\n",
    "            mapping_original_round_appear_in_pert[original_round_uid].append(id_pert_round)\n",
    "\n",
    "\n",
    "def get_original_round_id(id_pert_round):\n",
    "    pert_type = id_pert_round.split(\"#pert#\")[0]\n",
    "    pert_dialogue_key = id_pert_round.split(\"#pert#\")[1]\n",
    "    pert_round_key = id_pert_round.split(\"#pert#\")[2]\n",
    "    this_pert_dataset = dataset_jsons[pert_type]\n",
    "    return pert_dialogue_key + \"##\" + this_pert_dataset[pert_dialogue_key][pert_round_key]['is_original_round']\n",
    "\n",
    "def get_following_round_ids(id_pert_round):\n",
    "    pert_type = id_pert_round.split(\"#pert#\")[0]\n",
    "    pert_dialogue_key = id_pert_round.split(\"#pert#\")[1]\n",
    "    pert_round_key = id_pert_round.split(\"#pert#\")[2]\n",
    "\n",
    "    this_pert_dataset = dataset_jsons[pert_type]\n",
    "    following_round_ids = []\n",
    "    for round_key in this_pert_dataset[pert_dialogue_key].keys():\n",
    "        if int(round_key.split(\" \")[1]) > int(pert_round_key.split(\" \")[1]):\n",
    "            following_round_ids.append(pert_type + \"#pert#\" + pert_dialogue_key + \"#pert#\" + round_key)\n",
    "    return following_round_ids\n",
    "\n",
    "def question_changed(id_pert_round):\n",
    "    pert_type = id_pert_round.split(\"#pert#\")[0]\n",
    "    pert_dialogue_key = id_pert_round.split(\"#pert#\")[1]\n",
    "    pert_round_key = id_pert_round.split(\"#pert#\")[2]\n",
    "    return dataset_jsons[pert_type][pert_dialogue_key][pert_round_key][\"Question\"] != dataset_jsons[pert_type][pert_dialogue_key][pert_round_key][\"Original_question\"]\n",
    "\n",
    "\n",
    "dict_pert_dialogue_usage_mrt1 = {} # O1-r3 : [P1-O1-r2, P2...], (pert_round_uid), expect output(P1-O1-r2) == answer(O1-r3)\n",
    "\n",
    "for original_round_id in all_original_round_ids:\n",
    "    dict_pert_dialogue_usage_mrt1[original_round_id] = []\n",
    "\n",
    "\n",
    "# unperturbed rounds after perturbed round will not be excluded\n",
    "for id_pert_round in all_pert_round_ids:\n",
    "    original_round_id = get_original_round_id(id_pert_round)\n",
    "    if question_changed(id_pert_round):\n",
    "        dict_pert_dialogue_usage_mrt1[original_round_id].append(id_pert_round)\n",
    "\n",
    "for original_round_id in dict_pert_dialogue_usage_mrt1.copy().keys():\n",
    "    if len(dict_pert_dialogue_usage_mrt1[original_round_id]) == 0:\n",
    "        del dict_pert_dialogue_usage_mrt1[original_round_id]\n",
    "\n",
    "\n",
    "for original_round_id in dict_pert_dialogue_usage_mrt1.copy().keys():\n",
    "    new_list = [item.lstrip(\"500_\") for item in dict_pert_dialogue_usage_mrt1[original_round_id]]\n",
    "    dict_pert_dialogue_usage_mrt1[original_round_id] = new_list\n",
    "\n",
    "\n",
    "all_lms = [\"qwen2_0B5\", \"qwen2_1B5\", \"qwen2_7B\", \"mistral03_7B\", \"llama3_8B\", \"gemma2_9B\"]\n",
    "\n",
    "dist_LLM_DS_name = {\"qwen2_0B5\":\"DS1\", \"qwen2_1B5\":\"DS2\", \"qwen2_7B\":\"DS3\", \"mistral03_7B\":\"DS4\", \"llama3_8B\":\"DS5\", \"gemma2_9B\":\"DS6\"}\n",
    "\n",
    "METAL_Perts = [\n",
    "    \"MR10_SNP_s06_synonym_replacement\",\n",
    "    \"MR11_SNP_s06_introduce_typos\",\n",
    "    \"MR12_SNP_s06_to_leet\",\n",
    "    \"MR13_SNP_s06_add_words\"]\n",
    "\n",
    "METAL_Perts_S97 = [\n",
    "    \"S97_MR10_SNP_s06_synonym_replacement\",\n",
    "    \"S97_MR11_SNP_s06_introduce_typos\",\n",
    "    \"S97_MR12_SNP_s06_to_leet\",\n",
    "    \"S97_MR13_SNP_s06_add_words\"]\n",
    "\n",
    "\n",
    "result_path = \"result/MTMT/{}/{}.pickle\"\n",
    "\n",
    "dataset_original = json.load(open(\"data/500_MR0_SNP_s06_original.json\", \"r\"))\n",
    "\n",
    "METAL_result_DataFrames = {}\n",
    "for chosen_lm in all_lms:\n",
    "    for i_chosen_pert in range(len(METAL_Perts)):\n",
    "        chosen_pert = METAL_Perts[i_chosen_pert]\n",
    "        \n",
    "        chosen_pert_s97 = METAL_Perts_S97[i_chosen_pert]\n",
    "        df_result = pd.read_pickle(result_path.format(chosen_lm, chosen_pert))\n",
    "        df_result_s97 = pd.read_pickle(result_path.format(chosen_lm, chosen_pert_s97))\n",
    "        df_result = pd.concat([df_result, df_result_s97], ignore_index=True)\n",
    "\n",
    "        df_result = df_result[df_result[\"role\"] == \"assistant\"]\n",
    "        df_result[\"score_semantic\"]=[item[0] if item else None for item in df_result[\"score_semantic\"]]\n",
    "        df_result[\"row_uid\"] = [ (df_result.loc[row, \"source_uuid\"] + \"##\" + str(df_result.loc[row,\"round\"])) for row in df_result.index]\n",
    "        df_result[\"id_pert_round\"] = [chosen_pert + \"#pert#\" + df_result.loc[row, \"source_uuid\"] + \"#pert#\" + \"Round \" + str(df_result.loc[row,\"round\"]) for row in df_result.index]\n",
    "        METAL_result_DataFrames[chosen_lm + \"_\" + chosen_pert] = copy.deepcopy(df_result)\n",
    "\n",
    "def get_original_answer(id_pert_round):\n",
    "    pert_dialogue_key = id_pert_round.split(\"#pert#\")[1]\n",
    "    original_round_key = id_pert_round.split(\"#pert#\")[2]\n",
    "    return dataset_original[pert_dialogue_key][original_round_key][\"Answer\"]\n",
    "\n",
    "def get_pert_round_answer(id_pert_round):\n",
    "    pert_type = id_pert_round.split(\"#pert#\")[0]\n",
    "    df_result = METAL_result_DataFrames[chosen_lm + \"_\" + pert_type]\n",
    "    return df_result[df_result[\"id_pert_round\"] == id_pert_round][\"content\"].values[0]\n",
    "\n",
    "def get_pert_round_expected_answer(id_pert_round):\n",
    "    pert_type = id_pert_round.split(\"#pert#\")[0]\n",
    "    df_result = METAL_result_DataFrames[chosen_lm + \"_\" + pert_type]\n",
    "    return df_result[df_result[\"id_pert_round\"] == id_pert_round][\"expected_answer\"].values[0][0]\n",
    "\n",
    "def METAL_MRT1_conflict_detector(id_pert_round):\n",
    "    pert_type = id_pert_round.split(\"#pert#\")[0]\n",
    "    df_result = METAL_result_DataFrames[chosen_lm + \"_\" + pert_type]\n",
    "    score = df_result[df_result[\"id_pert_round\"] == id_pert_round][\"score_semantic\"].values[0]\n",
    "\n",
    "    BUG_FLAG=False\n",
    "    if score < 0.6:\n",
    "        BUG_FLAG=True\n",
    "    ERROR = abs((1 - score)/2)\n",
    "    return BUG_FLAG, ERROR\n",
    "\n",
    "PAPER_RESULT_METAL={}\n",
    "FOUNDED_Bugs_METAL = {}\n",
    "\n",
    "for chosen_lm in all_lms[:]:\n",
    "\n",
    "    MR1_TOOGLE = True\n",
    "    MR3_TOOGLE = True\n",
    "\n",
    "    METAL_Bugs = 0\n",
    "    METAL_ETC = []\n",
    "    METAL_BTC = 0\n",
    "    METAL_BSV_sum = 0\n",
    "    METAL_Amount_detection = []\n",
    "\n",
    "    # MR1\n",
    "    MR1_Amount_detection = []\n",
    "    MR1_Bugs = []\n",
    "    MR1_ETC = []\n",
    "    MR1_BSV_sum = 0\n",
    "\n",
    "    if MR1_TOOGLE:\n",
    "        print(\"MR1\")\n",
    "        for original_round_id in tqdm(dict_pert_dialogue_usage_mrt1.keys()):\n",
    "            original_dialogue_key = original_round_id.split(\"##\")[0]\n",
    "            original_round_key = original_round_id.split(\"##\")[1]\n",
    "\n",
    "            for pert_round_id in dict_pert_dialogue_usage_mrt1[original_round_id]:\n",
    "                pert_type = pert_round_id.split(\"#pert#\")[0]\n",
    "                pert_dialogue_key = pert_round_id.split(\"#pert#\")[1]\n",
    " \n",
    "                BUG_FLAG, ERROR = METAL_MRT1_conflict_detector(pert_round_id)\n",
    "                id_MR_check = \"MR1\" + \"#MRCheck#\" + pert_round_id\n",
    "                MR1_Amount_detection.append(id_MR_check)\n",
    "                if BUG_FLAG:\n",
    "                    id_bug = \"MR1#mr#\" + original_round_id + \"#bug#\" + pert_type + \"#pert#\" + pert_dialogue_key\n",
    "                    MR1_Bugs.append(id_bug)\n",
    "                    MR1_ETC.append(pert_dialogue_key)\n",
    "                    MR1_BSV_sum += ERROR\n",
    "\n",
    "    METAL_Bug_founded = MR1_Bugs\n",
    "    METAL_Bugs = len(METAL_Bug_founded)\n",
    "    METAL_ETC_ids = set(MR1_ETC)\n",
    "    METAL_ETC = round(len(METAL_ETC_ids)/500*100, 1) # percentage\n",
    "    METAL_BTC = METAL_Bugs/len(METAL_ETC_ids)\n",
    "    METAL_BTS = METAL_Bugs/500\n",
    "    METAL_BSV = (MR1_BSV_sum)/METAL_Bugs\n",
    "    METAL_Amount_detection = len(MR1_Amount_detection)\n",
    "    METAL_Rate_Plus = round(METAL_Bugs/METAL_Amount_detection*100, 1) \n",
    "    METAL_BugScore = METAL_BSV * METAL_BTS\n",
    "\n",
    "    PAPER_RESULT_METAL[chosen_lm] = {\"Bugs\":METAL_Bugs, \"ETC\":METAL_ETC, \"BTC\":round(METAL_BTC,3), \"BSV\":round(METAL_BSV,3), \"Rate+\":METAL_Rate_Plus, \"BugScore\":round(METAL_BugScore,3)}\n",
    "    FOUNDED_Bugs_METAL[chosen_lm] = METAL_Bug_founded\n",
    "\n",
    "    print(\"{} & {}  & {}\\\\%  & {}  & {}  & {}\\\\% & {} \\\\\\\\ \".format(dist_LLM_DS_name[chosen_lm], METAL_Bugs, METAL_ETC, round(METAL_BTC,3), round(METAL_BSV,3), METAL_Rate_Plus, round(METAL_BugScore,3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a494b9",
   "metadata": {},
   "source": [
    "### Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e8ba85df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"RQ1_PAPER_RESULT_METAL.pickle\", \"rb\") as f:\n",
    "    PAPER_RESULT_METAL = pickle.load(f)\n",
    "with open(\"RQ1_FOUNDED_Bugs_METAL.pickle\", \"rb\") as f:\n",
    "    FOUNDED_Bugs_METAL = pickle.load(f) \n",
    "with open(\"RQ1_PAPER_RESULT_MORTAR.pickle\", \"rb\") as f:\n",
    "    PAPER_RESULT_MORTAR = pickle.load(f)\n",
    "with open(\"RQ1_FOUNDED_Bugs_MORTAR.pickle\", \"rb\") as f:\n",
    "    FOUNDED_Bugs_MORTAR = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a28bc178",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DS1 & 12652  & 100.0\\%  & 25.304  & 86.3\\%  & 31942  & 100.0\\%  & 79.261 & 86.5\\% \\\\ \n",
      "DS2 & 10710  & 100.0\\%  & 21.42  & 73.0\\%  & 27799  & 100.0\\%  & 68.98 & 75.3\\% \\\\ \n",
      "DS3 & 9754  & 100.0\\%  & 19.508  & 66.5\\%  & 24634  & 100.0\\%  & 61.127 & 66.7\\% \\\\ \n",
      "DS4 & 9876  & 100.0\\%  & 19.752  & 67.3\\%  & 22919  & 100.0\\%  & 56.871 & 62.1\\% \\\\ \n",
      "DS5 & 6631  & 99.8\\%  & 13.289  & 45.2\\%  & 17264  & 100.0\\%  & 42.839 & 46.8\\% \\\\ \n",
      "DS6 & 4199  & 98.2\\%  & 8.552  & 28.6\\%  & 12805  & 99.8\\%  & 31.853 & 34.7\\% \\\\ \n",
      "Average & 8970  & 99.7\\%  & 17.971  & 61.2\\%  & 22893  & 100.0\\%  & 56.822    & 62.0\\%   \\\\ \n"
     ]
    }
   ],
   "source": [
    "all_lms = [\"qwen2_0B5\", \"qwen2_1B5\", \"qwen2_7B\", \"mistral03_7B\", \"llama3_8B\", \"gemma2_9B\"]\n",
    "\n",
    "for chosen_lm in all_lms:\n",
    "    print(\"{} & {}  & {}\\\\%  & {}  & {}\\\\%  & {}  & {}\\\\%  & {} & {}\\\\% \\\\\\\\ \".format(\n",
    "        dist_LLM_DS_name[chosen_lm],\n",
    "        PAPER_RESULT_METAL[chosen_lm][\"Bugs\"], \n",
    "        PAPER_RESULT_METAL[chosen_lm][\"ETC\"], \n",
    "        PAPER_RESULT_METAL[chosen_lm][\"BTC\"], \n",
    "        PAPER_RESULT_METAL[chosen_lm][\"Rate+\"], \n",
    "        PAPER_RESULT_MORTAR[chosen_lm][\"Bugs\"], \n",
    "        PAPER_RESULT_MORTAR[chosen_lm][\"ETC\"], \n",
    "        PAPER_RESULT_MORTAR[chosen_lm][\"BTC\"],  \n",
    "        PAPER_RESULT_MORTAR[chosen_lm][\"Rate+\"]))\n",
    "    \n",
    "print(\"Average & {}  & {}\\\\%  & {}  & {}\\\\%  & {}  & {}\\\\%  & {}    & {}\\\\%   \\\\\\\\ \".format(\n",
    "    int(np.mean([PAPER_RESULT_METAL[llm][\"Bugs\"] for llm in all_lms])),\n",
    "    round(np.mean([PAPER_RESULT_METAL[llm][\"ETC\"] for llm in all_lms]), 1),\n",
    "    round(np.mean([PAPER_RESULT_METAL[llm][\"BTC\"] for llm in all_lms]), 3),\n",
    "    # round(np.mean([PAPER_RESULT_METAL[llm][\"BSV\"] for llm in all_lms]), 3),\n",
    "    round(np.mean([PAPER_RESULT_METAL[llm][\"Rate+\"] for llm in all_lms]), 1),\n",
    "    # round(np.mean([PAPER_RESULT_METAL[llm][\"BugScore\"] for llm in all_lms]), 3),\n",
    "    \n",
    "    int(np.mean([PAPER_RESULT_MORTAR[llm][\"Bugs\"] for llm in all_lms])),\n",
    "    round(np.mean([PAPER_RESULT_MORTAR[llm][\"ETC\"] for llm in all_lms]), 1),\n",
    "    round(np.mean([PAPER_RESULT_MORTAR[llm][\"BTC\"] for llm in all_lms]), 3),\n",
    "    # round(np.mean([PAPER_RESULT_MORTAR[llm][\"BSV\"] for llm in all_lms]), 3),\n",
    "    round(np.mean([PAPER_RESULT_MORTAR[llm][\"Rate+\"] for llm in all_lms]), 1),\n",
    "    # round(np.mean([PAPER_RESULT_MORTAR[llm][\"BugScore\"] for llm in all_lms]), 3)\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddd29360",
   "metadata": {},
   "source": [
    "# RQ2: Bug Quality"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db06614",
   "metadata": {},
   "source": [
    "## 2.1 Diversity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ac8e43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DS1 & 10903 & 1749 &0 & 12652 & 27723 &4219 &0 & 31942 \\\\ \n",
      "DS2 & 8103 & 2607 &0 & 10710 & 21504 &6295 &0 & 27799 \\\\ \n",
      "DS3 & 7723 & 2000 &31 & 9754 & 19083 &5399 &152 & 24634 \\\\ \n",
      "DS4 & 7463 & 2413 &0 & 9876 & 18716 &4203 &0 & 22919 \\\\ \n",
      "DS5 & 4052 & 2490 &89 & 6631 & 10355 &6664 &245 & 17264 \\\\ \n",
      "DS6 & 2555 & 1573 &71 & 4199 & 6819 &5756 &230 & 12805 \\\\ \n",
      "\\midrule\n",
      "Average & 6799 & 2138 &31 & 53822 & 17366 &5422 &104 & 137363 \\\\ \n"
     ]
    }
   ],
   "source": [
    "df_MORTAR_bugs = pd.read_pickle(\"RQ1_FOUNDED_Bugs_MORTAR_with_bug_type.pickle\")\n",
    "df_METAL_bugs = pd.read_pickle(\"RQ1_FOUNDED_Bugs_METAL_with_bug_type.pickle\")\n",
    "\n",
    "all_lms = [\"qwen2_0B5\", \"qwen2_1B5\", \"qwen2_7B\", \"mistral03_7B\", \"llama3_8B\", \"gemma2_9B\"]\n",
    "dist_LLM_DS_name = {\"qwen2_0B5\":\"DS1\", \"qwen2_1B5\":\"DS2\", \"qwen2_7B\":\"DS3\", \"mistral03_7B\":\"DS4\", \"llama3_8B\":\"DS5\", \"gemma2_9B\":\"DS6\"}\n",
    "\n",
    "for llm in all_lms:\n",
    "    this_llm_l1_MORTAR = len(df_MORTAR_bugs[(df_MORTAR_bugs[\"SUT\"] == llm) & (df_MORTAR_bugs[\"bug_type\"] == 1)])\n",
    "    this_llm_l2_MORTAR = len(df_MORTAR_bugs[(df_MORTAR_bugs[\"SUT\"] == llm) & (df_MORTAR_bugs[\"bug_type\"] == 2)])\n",
    "    this_llm_l3_MORTAR = len(df_MORTAR_bugs[(df_MORTAR_bugs[\"SUT\"] == llm) & (df_MORTAR_bugs[\"bug_type\"] == 3)])\n",
    "\n",
    "    this_llm_MORTAR_total = len(df_MORTAR_bugs[df_MORTAR_bugs[\"SUT\"] == llm])\n",
    "\n",
    "    this_llm_l1_METAL = len(df_METAL_bugs[(df_METAL_bugs[\"SUT\"] == llm) & (df_METAL_bugs[\"bug_type\"] == 1)])\n",
    "    this_llm_l2_METAL = len(df_METAL_bugs[(df_METAL_bugs[\"SUT\"] == llm) & (df_METAL_bugs[\"bug_type\"] == 2)])\n",
    "    this_llm_l3_METAL = len(df_METAL_bugs[(df_METAL_bugs[\"SUT\"] == llm) & (df_METAL_bugs[\"bug_type\"] == 3)])\n",
    "    \n",
    "    this_llm_METAL_total = len(df_METAL_bugs[df_METAL_bugs[\"SUT\"] == llm])\n",
    "\n",
    "    print(\"{} & {} & {} &{} & {} & {} &{} &{} & {} \\\\\\\\ \".format(\n",
    "        dist_LLM_DS_name[llm],\n",
    "        this_llm_l1_METAL,\n",
    "        # round(this_llm_l1_METAL/this_llm_METAL_total*100,1),\n",
    "        this_llm_l2_METAL,\n",
    "        # round(this_llm_l2_METAL/this_llm_METAL_total*100,1),\n",
    "        this_llm_l3_METAL,\n",
    "        # round(this_llm_l3_METAL/this_llm_METAL_total*100,1),\n",
    "        this_llm_METAL_total,\n",
    "        this_llm_l1_MORTAR,\n",
    "        # round(this_llm_l1_MORTAR/this_llm_MORTAR_total*100,1),\n",
    "        this_llm_l2_MORTAR,\n",
    "        # round(this_llm_l2_MORTAR/this_llm_MORTAR_total*100,1),\n",
    "        this_llm_l3_MORTAR,\n",
    "        # round(this_llm_l3_MORTAR/this_llm_MORTAR_total*100,1),\n",
    "        this_llm_MORTAR_total\n",
    "    ))\n",
    "print(\"\\\\midrule\")\n",
    "\n",
    "print(\"Average & {} & {} &{} & {} & {} &{} &{} & {} \\\\\\\\ \".format(\n",
    "    int(df_METAL_bugs[\"bug_type\"].value_counts()[1]/6),\n",
    "    int(df_METAL_bugs[\"bug_type\"].value_counts()[2]/6),\n",
    "    int(df_METAL_bugs[\"bug_type\"].value_counts()[3]/6),\n",
    "    len(df_METAL_bugs),\n",
    "    int(df_MORTAR_bugs[\"bug_type\"].value_counts()[1]/6),\n",
    "    int(df_MORTAR_bugs[\"bug_type\"].value_counts()[2]/6),\n",
    "    int(df_MORTAR_bugs[\"bug_type\"].value_counts()[3]/6),\n",
    "    len(df_MORTAR_bugs)\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f2ae7177",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DS1: METAL = 1.023, MORTAR = 1.041\n",
      "DS2: METAL = 0.726, MORTAR = 0.774\n",
      "DS3: METAL = 0.832, MORTAR = 0.790\n",
      "DS4: METAL = 0.723, MORTAR = 0.896\n",
      "DS5: METAL = 0.338, MORTAR = 0.307\n",
      "DS6: METAL = 0.336, MORTAR = 0.120\n",
      "METAL_sum = 0.663\n",
      "MORTAR_sum = 0.654\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def coefficient_of_variation(data, *, sample=True):\n",
    "    values = np.asarray(data, dtype=float)\n",
    "    mean = values.mean()\n",
    "    ddof = 1 if sample else 0\n",
    "    std = values.std(ddof=ddof)\n",
    "    return std / mean if mean != 0 else np.nan\n",
    "\n",
    "data = [\n",
    "    [10903 , 1749 ,0 , 12652 , 27723 ,4219 ,0 , 31942 ],\n",
    "    [8103 , 2607 ,0 , 10710 , 21504 ,6295 ,0 , 27799 ],\n",
    "    [7723 , 2000 ,31 , 9754 , 19083 ,5399 ,152 , 24634 ],\n",
    "    [7463 , 2413 ,0 , 9876 , 18716 ,4203 ,0 , 22919 ],\n",
    "    [4052 , 2490 ,89 , 6631 , 10355 ,6664 ,245 , 17264 ],\n",
    "    [2555 , 1573 ,71 , 4199 , 6819 ,5756 ,230 , 12805]\n",
    "    ]\n",
    "\n",
    "DS = [\"DS1\", \"DS2\", \"DS3\", \"DS4\", \"DS5\", \"DS6\"]\n",
    "\n",
    "METAL_sum = 0\n",
    "MORTAR_sum = 0\n",
    "\n",
    "for i in range(6):\n",
    "    METAL = coefficient_of_variation(data[i][0:2])\n",
    "    MORTAR = coefficient_of_variation(data[i][4:6])\n",
    "    print(f\"{DS[i]}: METAL = {METAL:.3f}, MORTAR = {MORTAR:.3f}\")   \n",
    "    METAL_sum += METAL\n",
    "    MORTAR_sum += MORTAR\n",
    "\n",
    "print(f\"METAL_sum = {METAL_sum/6:.3f}\")\n",
    "print(f\"MORTAR_sum = {MORTAR_sum/6:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57feabef",
   "metadata": {},
   "source": [
    "## 2.2 Precision (Manual Check)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c44b5d1",
   "metadata": {},
   "source": [
    "### MORTAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f98002c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "dataset_dev_full= json.load(open(\"data/extracted_dev_all_final_update_3.json\"))\n",
    "dataset_original = json.load(open(\"data/500_MR0_SNP_s06_original.json\", \"r\"))\n",
    "\n",
    "import json \n",
    "import pandas as pd\n",
    "import copy\n",
    "\n",
    "# MORTAR dataset\n",
    "MORTAR_Perts = [\n",
    "    \"P1_round_shuffle\",\n",
    "    \"P2_round_reduce\",\n",
    "    \"P3_round_duplicate\",\n",
    "    \"P4_round_reduce_shuffle\",\n",
    "    \"P5_round_shuffle_duplicate\",\n",
    "    ]\n",
    "MORTAR_dataset_jsons = {}\n",
    "for this_pert in MORTAR_Perts:\n",
    "    with open(f\"data/{this_pert}.json\", \"r\") as f:\n",
    "        MORTAR_dataset_jsons[this_pert] = json.load(f)\n",
    "\n",
    "\n",
    "# MORTAR result\n",
    "all_lms = [\"qwen2_0B5\", \"qwen2_1B5\", \"qwen2_7B\", \"mistral03_7B\", \"llama3_8B\", \"gemma2_9B\"]\n",
    "result_path = \"result/MTMT/{}/{}.pickle\"\n",
    "dataset_original = json.load(open(\"data/500_MR0_SNP_s06_original.json\", \"r\"))\n",
    "MORTAR_result_DataFrames = {}\n",
    "for chosen_lm in all_lms:\n",
    "    for chosen_pert in MORTAR_Perts:\n",
    "        df_result = pd.read_pickle(result_path.format(chosen_lm, chosen_pert))\n",
    "        df_result = df_result[df_result[\"role\"] == \"assistant\"]\n",
    "        df_result[\"score_semantic\"]=[item[0] if item else None for item in df_result[\"score_semantic\"]]\n",
    "        df_result[\"row_uid\"] = [ (df_result.loc[row, \"source_uuid\"] + \"##\" + str(df_result.loc[row,\"round\"])) for row in df_result.index]\n",
    "        df_result[\"id_pert_round\"] = [chosen_pert + \"#pert#\" + df_result.loc[row, \"source_uuid\"] + \"#pert#\" + \"Round \" + str(df_result.loc[row,\"round\"]) for row in df_result.index]\n",
    "        MORTAR_result_DataFrames[chosen_lm + \"_\" + chosen_pert] = copy.deepcopy(df_result)\n",
    "\n",
    "def get_original_story(dialogue_key):\n",
    "    return dataset_dev_full[dialogue_key][\"story_material\"]\n",
    "\n",
    "def get_original_qa(dialogue_key):\n",
    "    return dataset_dev_full[dialogue_key]['combined']\n",
    "\n",
    "def get_perturbed_qa_MORTAR(dialogue_key, pert):\n",
    "    return MORTAR_dataset_jsons[pert][dialogue_key]\n",
    "\n",
    "def pert_to_readable(pert_rounds):\n",
    "    result = \"\"\n",
    "    all_rounds = pert_rounds.keys()\n",
    "    for round in all_rounds:\n",
    "        result += f\"{round} - {pert_rounds[round]['Question']} \\n {round} - {pert_rounds[round]['Answer']}\\n\"\n",
    "    return result\n",
    "\n",
    "df_MORTAR_bugs = pd.read_pickle(\"RQ1_FOUNDED_Bugs_MORTAR_df.pickle\")\n",
    "df_MORTAR_check_bugs = df_MORTAR_bugs.sample(100)\n",
    "df_MORTAR_check_bugs.sort_values(by=[\"dialogue_key\", \"original_round_num\", \"SUT\"], inplace=True)\n",
    "\n",
    "temp_container = []\n",
    "for index in df_MORTAR_check_bugs.index:\n",
    "    row_bug = df_MORTAR_check_bugs.loc[index]\n",
    "    bug_id = row_bug[\"bug_id\"]\n",
    "    use_mr = row_bug[\"use_mr\"]\n",
    "    use_pert = row_bug[\"use_pert_1\"]\n",
    "    dialogue_key = row_bug[\"dialogue_key\"]\n",
    "    story = get_original_story(dialogue_key)\n",
    "    original_qa = get_original_qa(dialogue_key)\n",
    "\n",
    "    target_round_1 = row_bug[\"pert_round_num_1\"]\n",
    "    pertuabed_1 = pert_to_readable(get_perturbed_qa_MORTAR(dialogue_key, row_bug[\"use_pert_1\"]))\n",
    "    pert_round_id = dialogue_key + \"##\" + target_round_1 # 3dr23u6we5exclen4th8uq9rb42tel##1\n",
    "    temp_df = MORTAR_result_DataFrames[row_bug[\"SUT\"] + \"_\" + use_pert]\n",
    "    llm_output_1 = temp_df[temp_df[\"row_uid\"] == pert_round_id][\"content\"].values[0]\n",
    "    expect_output_1 = temp_df[temp_df[\"row_uid\"] == pert_round_id][\"expected_answer\"].values[0][0]\n",
    "\n",
    "    if use_mr in [\"MR1\", \"MR2\"]:\n",
    "        temp_container.append({\"bug_id\":bug_id,\n",
    "                               \"dialogue_key\":dialogue_key,\n",
    "                               \"story\":story,\n",
    "                               \"story_cn\":None,\n",
    "                               \"original_qa\":original_qa,\n",
    "                               \"pertuabed_1\":pertuabed_1,\n",
    "                               \"target_round_1\":target_round_1,\n",
    "                               \"llm_output_1\":llm_output_1,\n",
    "                               \"expect_output_1\":expect_output_1,\n",
    "                                \"pertuabed_2\":None,\n",
    "                                \"target_round_2\":None,\n",
    "                                \"llm_output_2\":None,\n",
    "                                \"expect_output_2\":None,\n",
    "                               \"note\": \"Answer should be both correct and short.\",\n",
    "                               \"isTruePositive\":None}),\n",
    "\n",
    "    elif use_mr in [\"MR3\",\"MR4\"]:\n",
    "        target_round_2 = row_bug[\"pert_round_num_2\"]\n",
    "        pertuabed_2 = pert_to_readable(get_perturbed_qa_MORTAR(dialogue_key, row_bug[\"use_pert_2\"]))\n",
    "        pert_round_id = dialogue_key + \"##\" + target_round_2\n",
    "        temp_df = MORTAR_result_DataFrames[row_bug[\"SUT\"] + \"_\" + row_bug[\"use_pert_2\"]]\n",
    "        llm_output_2 = temp_df[temp_df[\"row_uid\"] == pert_round_id][\"content\"].values[0]\n",
    "        expect_output_2 = temp_df[temp_df[\"row_uid\"] == pert_round_id][\"expected_answer\"].values[0][0]\n",
    "        temp_container.append({\"bug_id\":bug_id,\n",
    "                               \"dialogue_key\":dialogue_key,\n",
    "                               \"story\":story,\n",
    "                               \"story_cn\":None,\n",
    "                               \"original_qa\":original_qa,\n",
    "                               \"pertuabed_1\":pertuabed_1,\n",
    "                               \"target_round_1\":target_round_1,\n",
    "                               \"llm_output_1\":llm_output_1,\n",
    "                               \"expect_output_1\":expect_output_1,\n",
    "                                \"pertuabed_2\":pertuabed_2,\n",
    "                                \"target_round_2\":target_round_2,\n",
    "                                \"llm_output_2\":llm_output_2,\n",
    "                                \"expect_output_2\":expect_output_2,\n",
    "                               \"note\": \"Answer should be both correct and short.\",\n",
    "                               \"isTruePositive\":None}),\n",
    "\n",
    "df_mc_MORTAR = pd.DataFrame(columns = [\"bug_id\",\"dialogue_key\", \"story\", \"story_cn\", \"original_qa\", \"pertuabed_1\", \"target_round_1\", \"llm_output_1\", \"expect_output_1\", \"pertuabed_2\", \"target_round_2\", \"llm_output_2\", \"expect_output_2\", \"note\",\"isTruePositive\"], data = temp_container)\n",
    "df_mc_MORTAR.to_excel(\"RQ142_FOUNDED_Bugs_MORTAR_manual_check.xlsx\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e979d82b",
   "metadata": {},
   "source": [
    "### METAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3346854b",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_lms = [\"qwen2_0B5\", \"qwen2_1B5\", \"qwen2_7B\", \"mistral03_7B\", \"llama3_8B\", \"gemma2_9B\"]\n",
    "\n",
    "# METAL dataset\n",
    "METAL_perts = [\n",
    "    \"500_MR10_SNP_s06_synonym_replacement\",\n",
    "    \"500_MR13_SNP_s06_add_words\",\n",
    "    \"500_MR11_SNP_s06_introduce_typos\",\n",
    "    \"500_MR12_SNP_s06_to_leet\",\n",
    "]\n",
    "\n",
    "METAL_dataset_jsons = {}\n",
    "\n",
    "for this_pert in METAL_perts:\n",
    "    with open(f\"data/{this_pert}.json\", \"r\") as f:\n",
    "        METAL_dataset_jsons[this_pert.lstrip(\"500_\")] = json.load(f)\n",
    "\n",
    "# result\n",
    "METAL_Perts = [\n",
    "    \"MR10_SNP_s06_synonym_replacement\",\n",
    "    \"MR11_SNP_s06_introduce_typos\",\n",
    "    \"MR12_SNP_s06_to_leet\",\n",
    "    \"MR13_SNP_s06_add_words\"]\n",
    "\n",
    "METAL_Perts_S97 = [\n",
    "    \"S97_MR10_SNP_s06_synonym_replacement\",\n",
    "    \"S97_MR11_SNP_s06_introduce_typos\",\n",
    "    \"S97_MR12_SNP_s06_to_leet\",\n",
    "    \"S97_MR13_SNP_s06_add_words\"]\n",
    "\n",
    "\n",
    "result_path = \"result/MTMT/{}/{}.pickle\"\n",
    "METAL_result_DataFrames = {}\n",
    "for chosen_lm in all_lms:\n",
    "    for i_chosen_pert in range(len(METAL_Perts)):\n",
    "        chosen_pert = METAL_Perts[i_chosen_pert]\n",
    "        \n",
    "        chosen_pert_s97 = METAL_Perts_S97[i_chosen_pert]\n",
    "        df_result = pd.read_pickle(result_path.format(chosen_lm, chosen_pert))\n",
    "        df_result_s97 = pd.read_pickle(result_path.format(chosen_lm, chosen_pert_s97))\n",
    "        df_result = pd.concat([df_result, df_result_s97], ignore_index=True)\n",
    "\n",
    "        df_result = df_result[df_result[\"role\"] == \"assistant\"]\n",
    "        df_result[\"score_semantic\"]=[item[0] if item else None for item in df_result[\"score_semantic\"]]\n",
    "        df_result[\"row_uid\"] = [ (df_result.loc[row, \"source_uuid\"] + \"##\" + str(df_result.loc[row,\"round\"])) for row in df_result.index]\n",
    "        df_result[\"id_pert_round\"] = [chosen_pert + \"#pert#\" + df_result.loc[row, \"source_uuid\"] + \"#pert#\" + \"Round \" + str(df_result.loc[row,\"round\"]) for row in df_result.index]\n",
    "        METAL_result_DataFrames[chosen_lm + \"_\" + chosen_pert] = copy.deepcopy(df_result)\n",
    "\n",
    "def get_original_story(dialogue_key):\n",
    "    return dataset_dev_full[dialogue_key][\"story_material\"]\n",
    "\n",
    "def get_original_qa(dialogue_key):\n",
    "    return dataset_dev_full[dialogue_key]['combined']\n",
    "\n",
    "def get_perturbed_qa_METAL(dialogue_key, pert):\n",
    "    return METAL_dataset_jsons[pert][dialogue_key]\n",
    "\n",
    "def pert_to_readable(pert_rounds):\n",
    "    result = \"\"\n",
    "    all_rounds = pert_rounds.keys()\n",
    "    for round in all_rounds:\n",
    "        result += f\"{round} - {pert_rounds[round]['Question']} \\n {round} - {pert_rounds[round]['Answer']}\\n\"\n",
    "    return result\n",
    "\n",
    "df_METAL_bugs = pd.read_pickle(\"RQ1_FOUNDED_Bugs_METAL_df.pickle\")\n",
    "df_METAL_check_bugs = df_METAL_bugs.sample(100)\n",
    "df_METAL_check_bugs.sort_values(by=[\"dialogue_key\", \"original_round_num\", \"SUT\"], inplace=True)\n",
    "\n",
    "temp_container = []\n",
    "for index in df_METAL_check_bugs.index:\n",
    "    row_bug = df_METAL_check_bugs.loc[index]\n",
    "    bug_id = row_bug[\"bug_id\"]\n",
    "    use_mr = row_bug[\"use_mr\"]\n",
    "    use_pert = row_bug[\"use_pert_1\"]\n",
    "    dialogue_key = row_bug[\"dialogue_key\"]\n",
    "    story = get_original_story(dialogue_key)\n",
    "    original_qa = get_original_qa(dialogue_key)\n",
    "\n",
    "    target_round_1 = row_bug[\"pert_round_num_1\"]\n",
    "    pertuabed_1 = pert_to_readable(get_perturbed_qa_METAL(dialogue_key, row_bug[\"use_pert_1\"]))\n",
    "    pert_round_id = dialogue_key + \"##\" + target_round_1 # 3dr23u6we5exclen4th8uq9rb42tel##1\n",
    "    temp_df = METAL_result_DataFrames[row_bug[\"SUT\"] + \"_\" + use_pert]\n",
    "    llm_output_1 = temp_df[temp_df[\"row_uid\"] == pert_round_id][\"content\"].values[0]\n",
    "    expect_output_1 = temp_df[temp_df[\"row_uid\"] == pert_round_id][\"expected_answer\"].values[0][0]\n",
    "\n",
    "    if use_mr in [\"MR1\", \"MR2\"]:\n",
    "        temp_container.append({\"bug_id\":bug_id,\n",
    "                               \"dialogue_key\":dialogue_key,\n",
    "                               \"story\":story,\n",
    "                               \"story_cn\":None,\n",
    "                               \"original_qa\":original_qa,\n",
    "                               \"pertuabed_1\":pertuabed_1,\n",
    "                               \"target_round_1\":target_round_1,\n",
    "                               \"llm_output_1\":llm_output_1,\n",
    "                               \"expect_output_1\":expect_output_1,\n",
    "                                \"pertuabed_2\":None,\n",
    "                                \"target_round_2\":None,\n",
    "                                \"llm_output_2\":None,\n",
    "                                \"expect_output_2\":None,\n",
    "                               \"note\": \"Answer should be both correct and short.\",\n",
    "                               \"isTruePositive\":None}),\n",
    "\n",
    "    elif use_mr in [\"MR3\",\"MR4\"]:\n",
    "        target_round_2 = row_bug[\"pert_round_num_2\"]\n",
    "        pertuabed_2 = pert_to_readable(get_perturbed_qa_METAL(dialogue_key, row_bug[\"use_pert_2\"]))\n",
    "        pert_round_id = dialogue_key + \"##\" + target_round_2\n",
    "        temp_df = METAL_result_DataFrames[row_bug[\"SUT\"] + \"_\" + row_bug[\"use_pert_2\"]]\n",
    "        llm_output_2 = temp_df[temp_df[\"row_uid\"] == pert_round_id][\"content\"].values[0]\n",
    "        expect_output_2 = temp_df[temp_df[\"row_uid\"] == pert_round_id][\"expected_answer\"].values[0][0]\n",
    "        temp_container.append({\"bug_id\":bug_id,\n",
    "                               \"dialogue_key\":dialogue_key,\n",
    "                               \"story\":story,\n",
    "                               \"story_cn\":None,\n",
    "                               \"original_qa\":original_qa,\n",
    "                               \"pertuabed_1\":pertuabed_1,\n",
    "                               \"target_round_1\":target_round_1,\n",
    "                               \"llm_output_1\":llm_output_1,\n",
    "                               \"expect_output_1\":expect_output_1,\n",
    "                                \"pertuabed_2\":pertuabed_2,\n",
    "                                \"target_round_2\":target_round_2,\n",
    "                                \"llm_output_2\":llm_output_2,\n",
    "                                \"expect_output_2\":expect_output_2,\n",
    "                               \"note\": \"Answer should be both correct and short.\",\n",
    "                               \"isTruePositive\":None}),\n",
    "\n",
    "df_mc_METAL = pd.DataFrame(columns = [\"bug_id\",\"dialogue_key\", \"story\", \"story_cn\", \"original_qa\", \"pertuabed_1\", \"target_round_1\", \"llm_output_1\", \"expect_output_1\", \"pertuabed_2\", \"target_round_2\", \"llm_output_2\", \"expect_output_2\", \"note\",\"isTruePositive\"], data = temp_container)\n",
    "df_mc_METAL.to_excel(\"RQ142_FOUNDED_Bugs_METAL_manual_check.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e135e76",
   "metadata": {},
   "source": [
    "### Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e33ab471",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MORTAR\n",
      "kappa: 0.8318924111431316\n",
      "TPR: 0.705\n",
      "METAL\n",
      "kappa: 0.7784045124899275\n",
      "TPR: 0.455\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, cohen_kappa_score, f1_score, matthews_corrcoef\n",
    "\n",
    "df_mc_MORTAR = pd.read_excel(\"data/manual_check/Merge_RQ142_FOUNDED_Bugs_MORTAR_manual_check.xlsx\")\n",
    "print(\"MORTAR\")\n",
    "print(\"kappa:\",cohen_kappa_score((df_mc_MORTAR[\"c1_llm_correct\"]<=6).to_list(), (df_mc_MORTAR[\"c2_llm_correct\"]<=6).to_list()))\n",
    "print(\"TPR:\",(len(df_mc_MORTAR[df_mc_MORTAR[\"c2_llm_correct\"]<=6])+len(df_mc_MORTAR[df_mc_MORTAR[\"c1_llm_correct\"]<=6]))/200)\n",
    "print(\"METAL\")\n",
    "df_mc_METAL = pd.read_excel(\"data/manual_check/Merge_RQ142_FOUNDED_Bugs_METAL_manual_check.xlsx\")\n",
    "print(\"kappa:\",cohen_kappa_score((df_mc_METAL[\"c1_llm_correct\"]<=6).to_list(), (df_mc_METAL[\"c2_llm_correct\"]<=6).to_list()))\n",
    "print(\"TPR:\",(len(df_mc_METAL[df_mc_METAL[\"c2_llm_correct\"]<=6])+len(df_mc_METAL[df_mc_METAL[\"c1_llm_correct\"]<=6]))/200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "553687a7",
   "metadata": {},
   "source": [
    "## 2.3 Uniqueness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49dca204",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUTs & L1-Bugs & L2-Bugs & L3-Bugs & Overall \\\\ \n",
      "\\midrule\n",
      "DS1 & 25.6\\% & 29.5\\% & 0\\% & 26.1\\% \\\\ \n",
      "DS2 & 30.0\\% & 35.3\\% & 0\\% & 31.2\\% \\\\ \n",
      "DS3 & 28.4\\% & 47.9\\% & 65.1\\% & 32.9\\% \\\\ \n",
      "DS4 & 30.9\\% & 35.8\\% & 0\\% & 31.8\\% \\\\ \n",
      "DS5 & 31.8\\% & 57.4\\% & 62.0\\% & 42.1\\% \\\\ \n",
      "DS6 & 36.5\\% & 71.6\\% & 77.0\\% & 53.0\\% \\\\ \n",
      "\\midrule\n",
      "Average & 29.3\\% & 47.6\\% & 68.3\\% & 33.8\\% \\\\\n",
      "SUTs & L1-Bugs & L2-Bugs & L3-Bugs & Overall \\\\ \n",
      "\\midrule\n",
      "DS1 & 20.5\\% & 22.9\\% & 0\\% & 20.9\\% \\\\ \n",
      "DS2 & 20.6\\% & 27.6\\% & 0\\% & 22.3\\% \\\\ \n",
      "DS3 & 20.2\\% & 32.1\\% & 35.5\\% & 22.7\\% \\\\ \n",
      "DS4 & 21.5\\% & 38.0\\% & 0\\% & 25.5\\% \\\\ \n",
      "DS5 & 22.9\\% & 43.6\\% & 53.9\\% & 31.1\\% \\\\ \n",
      "DS6 & 24.4\\% & 49.8\\% & 63.4\\% & 34.6\\% \\\\ \n",
      "\\midrule\n",
      "Average & 21.1\\% & 35.4\\% & 54.5\\% & 24.7\\% \\\\\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_MORTAR_bugs = pd.read_pickle(\"RQ1_FOUNDED_Bugs_MORTAR_with_bug_type.pickle\")\n",
    "df_MORTAR_bugs[\"bug_round_seed\"] = df_MORTAR_bugs[\"dialogue_key\"] + \"##\" + df_MORTAR_bugs[\"original_round_num\"]\n",
    "df_METAL_bugs = pd.read_pickle(\"RQ1_FOUNDED_Bugs_METAL_with_bug_type.pickle\")\n",
    "df_METAL_bugs[\"bug_round_seed\"] = df_METAL_bugs[\"dialogue_key\"] + \"##\" + df_METAL_bugs[\"original_round_num\"]\n",
    "\n",
    "bug_levels = [1,2,3]\n",
    "all_lms = [\"qwen2_0B5\", \"qwen2_1B5\", \"qwen2_7B\", \"mistral03_7B\", \"llama3_8B\", \"gemma2_9B\"]\n",
    "dist_LLM_DS_name = {\"qwen2_0B5\":\"DS1\", \"qwen2_1B5\":\"DS2\", \"qwen2_7B\":\"DS3\", \"mistral03_7B\":\"DS4\", \"llama3_8B\":\"DS5\", \"gemma2_9B\":\"DS6\"}\n",
    "\n",
    "venn_container = {}\n",
    "\n",
    "MORTAR_unique_l1_bugs = []\n",
    "MORTAR_unique_l2_bugs = []\n",
    "MORTAR_unique_l3_bugs = []\n",
    "\n",
    "METAL_unique_l1_bugs = []\n",
    "METAL_unique_l2_bugs = []\n",
    "METAL_unique_l3_bugs = []\n",
    "\n",
    "for llm in all_lms:\n",
    "    venn_container[llm] = {}\n",
    "    for bug_level in bug_levels:\n",
    "        venn_container[llm][bug_level] = {\"MORTAR\":[], \"METAL\":[], \"Overlap\":[]}\n",
    "\n",
    "        df_MORTAR_llm_this_buglevel = df_MORTAR_bugs[\n",
    "            (df_MORTAR_bugs[\"bug_type\"] == bug_level) &\n",
    "            (df_MORTAR_bugs[\"SUT\"] == llm)\n",
    "            ]\n",
    "        list_MORTAR_llm_this_buglevel_bug_round_seed = df_MORTAR_llm_this_buglevel[\"bug_round_seed\"].tolist()\n",
    "        venn_container[llm][bug_level][\"MORTAR\"] = list_MORTAR_llm_this_buglevel_bug_round_seed\n",
    "\n",
    "        df_METAL_llm_this_buglevel = df_METAL_bugs[\n",
    "            (df_METAL_bugs[\"bug_type\"] == bug_level) &\n",
    "            (df_METAL_bugs[\"SUT\"] == llm)\n",
    "            ]\n",
    "        list_METAL_llm_this_buglevel_bug_round_seed = df_METAL_llm_this_buglevel[\"bug_round_seed\"].tolist()\n",
    "        venn_container[llm][bug_level][\"METAL\"] = list_METAL_llm_this_buglevel_bug_round_seed\n",
    "\n",
    "        venn_container[llm][bug_level][\"Overlap\"] = [item for item in list_MORTAR_llm_this_buglevel_bug_round_seed if item in list_METAL_llm_this_buglevel_bug_round_seed]\n",
    "\n",
    "        this_llm_this_level_mortar_unique_bugs = [item for item in list_MORTAR_llm_this_buglevel_bug_round_seed if item not in list_METAL_llm_this_buglevel_bug_round_seed]\n",
    "        this_llm_this_level_metal_unique_bugs = [item for item in list_METAL_llm_this_buglevel_bug_round_seed if item not in list_MORTAR_llm_this_buglevel_bug_round_seed]\n",
    "\n",
    "        if bug_level == 1:\n",
    "            MORTAR_unique_l1_bugs += this_llm_this_level_mortar_unique_bugs\n",
    "            METAL_unique_l1_bugs += this_llm_this_level_metal_unique_bugs\n",
    "        elif bug_level == 2:\n",
    "            MORTAR_unique_l2_bugs += this_llm_this_level_mortar_unique_bugs\n",
    "            METAL_unique_l2_bugs += this_llm_this_level_metal_unique_bugs\n",
    "        elif bug_level == 3:\n",
    "            MORTAR_unique_l3_bugs += this_llm_this_level_mortar_unique_bugs\n",
    "            METAL_unique_l3_bugs += this_llm_this_level_metal_unique_bugs\n",
    "\n",
    "# MORTAR\n",
    "\n",
    "print(\"SUTs & L1-Bugs & L2-Bugs & L3-Bugs & Overall \\\\\\\\ \")\n",
    "print(\"\\\\midrule\")\n",
    "\n",
    "for llm in all_lms:\n",
    "    mortar_l1_unique_bugs = [item for item in venn_container[llm][1][\"MORTAR\"] if item not in venn_container[llm][1][\"Overlap\"]]\n",
    "    mortar_l2_unique_bugs = [item for item in venn_container[llm][2][\"MORTAR\"] if item not in venn_container[llm][2][\"Overlap\"]]\n",
    "    mortar_l3_unique_bugs = [item for item in venn_container[llm][3][\"MORTAR\"] if item not in venn_container[llm][3][\"Overlap\"]]\n",
    "    print(\"{} & {}\\% & {}\\% & {}\\% & {}\\% \\\\\\\\ \".format(\n",
    "        dist_LLM_DS_name[llm],\n",
    "        round(len(mortar_l1_unique_bugs)/len(venn_container[llm][1][\"MORTAR\"])*100, 1),\n",
    "        round(len(mortar_l2_unique_bugs)/len(venn_container[llm][2][\"MORTAR\"])*100, 1),\n",
    "        round(len(mortar_l3_unique_bugs)/len(venn_container[llm][3][\"MORTAR\"])*100, 1) if len(venn_container[llm][3][\"MORTAR\"])!=0 else 0,\n",
    "        round((len(mortar_l1_unique_bugs) + len(mortar_l2_unique_bugs) + len(mortar_l3_unique_bugs))/(len(venn_container[llm][1][\"MORTAR\"]) + len( venn_container[llm][2][\"MORTAR\"]) + len(venn_container[llm][3][\"MORTAR\"]))*100, 1)\n",
    "    ))\n",
    "\n",
    "print(\"\\\\midrule\")\n",
    "print(\"Average & {}\\% & {}\\% & {}\\% & {}\\% \\\\\\\\\".format(\n",
    "    round(len(MORTAR_unique_l1_bugs)/len(df_MORTAR_bugs[df_MORTAR_bugs[\"bug_type\"] == 1])*100,1),\n",
    "    round(len(MORTAR_unique_l2_bugs)/len(df_MORTAR_bugs[df_MORTAR_bugs[\"bug_type\"] == 2])*100,1),\n",
    "    round(len(MORTAR_unique_l3_bugs)/len(df_MORTAR_bugs[df_MORTAR_bugs[\"bug_type\"] == 3])*100,1),\n",
    "    round((len(MORTAR_unique_l1_bugs) + len(MORTAR_unique_l2_bugs) + len(MORTAR_unique_l3_bugs))/(len(df_MORTAR_bugs[df_MORTAR_bugs[\"bug_type\"] == 1]) + len(df_MORTAR_bugs[df_MORTAR_bugs[\"bug_type\"] == 2]) + len(df_MORTAR_bugs[df_MORTAR_bugs[\"bug_type\"] == 3]))*100,1)\n",
    "))\n",
    "\n",
    "# METAL\n",
    "print(\"SUTs & L1-Bugs & L2-Bugs & L3-Bugs & Overall \\\\\\\\ \")\n",
    "print(\"\\\\midrule\")\n",
    "\n",
    "for llm in all_lms:\n",
    "    metal_l1_unique_bugs = [item for item in venn_container[llm][1][\"METAL\"] if item not in venn_container[llm][1][\"Overlap\"]]\n",
    "    metal_l2_unique_bugs = [item for item in venn_container[llm][2][\"METAL\"] if item not in venn_container[llm][2][\"Overlap\"]]\n",
    "    metal_l3_unique_bugs = [item for item in venn_container[llm][3][\"METAL\"] if item not in venn_container[llm][3][\"Overlap\"]]\n",
    "    print(\"{} & {}\\% & {}\\% & {}\\% & {}\\% \\\\\\\\ \".format(\n",
    "        dist_LLM_DS_name[llm],\n",
    "        round(len(metal_l1_unique_bugs)/len(venn_container[llm][1][\"METAL\"])*100, 1),\n",
    "        round(len(metal_l2_unique_bugs)/len(venn_container[llm][2][\"METAL\"])*100, 1),\n",
    "        round(len(metal_l3_unique_bugs)/len(venn_container[llm][3][\"METAL\"])*100, 1) if len(venn_container[llm][3][\"METAL\"])!=0 else 0,\n",
    "        round((len(metal_l1_unique_bugs) + len(metal_l2_unique_bugs) + len(metal_l3_unique_bugs))/(len(venn_container[llm][1][\"METAL\"]) + len( venn_container[llm][2][\"METAL\"]) + len(venn_container[llm][3][\"METAL\"]))*100, 1)\n",
    "    ))\n",
    "\n",
    "print(\"\\\\midrule\")\n",
    "print(\"Average & {}\\% & {}\\% & {}\\% & {}\\% \\\\\\\\\".format(\n",
    "    round(len(METAL_unique_l1_bugs)/len(df_METAL_bugs[df_METAL_bugs[\"bug_type\"] == 1])*100,1),\n",
    "    round(len(METAL_unique_l2_bugs)/len(df_METAL_bugs[df_METAL_bugs[\"bug_type\"] == 2])*100,1),\n",
    "    round(len(METAL_unique_l3_bugs)/len(df_METAL_bugs[df_METAL_bugs[\"bug_type\"] == 3])*100,1),\n",
    "    round((len(METAL_unique_l1_bugs) + len(METAL_unique_l2_bugs) + len(METAL_unique_l3_bugs))/(len(df_METAL_bugs[df_METAL_bugs[\"bug_type\"] == 1]) + len(df_METAL_bugs[df_METAL_bugs[\"bug_type\"] == 2]) + len(df_METAL_bugs[df_METAL_bugs[\"bug_type\"] == 3]))*100,1)\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "140faee4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "METAL SUTs & L1-Bugs & L2-Bugs & L3-Bugs & Overall \\\\ \n",
      "\\midrule\n",
      "DS1 & 1.0\\% & 2.7\\% & 0\\% & 1.2\\% \\\\ \n",
      "DS2 & 1.3\\% & 7.8\\% & 0\\% & 2.9\\% \\\\ \n",
      "DS3 & 1.5\\% & 17.9\\% & 13.0\\% & 5.0\\% \\\\ \n",
      "DS4 & 2.6\\% & 22.6\\% & 0\\% & 7.5\\% \\\\ \n",
      "DS5 & 4.9\\% & 30.4\\% & 38.8\\% & 14.9\\% \\\\ \n",
      "DS6 & 5.5\\% & 38.9\\% & 43.5\\% & 18.8\\% \\\\ \n",
      "\\midrule\n",
      "Average & 2.1\\% & 19.8\\% & 36.0\\% & 6.5\\% \\\\\n"
     ]
    }
   ],
   "source": [
    "# unique 403\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "all_403_dialogues = json.load(open(\"data/dialogue_round_keys_403.json\", \"r\"))\n",
    "all_403_dialogues.keys()\n",
    "\n",
    "\n",
    "df_MORTAR_bugs_all = pd.read_pickle(\"RQ1_FOUNDED_Bugs_MORTAR_with_bug_type.pickle\")\n",
    "df_MORTAR_bugs = df_MORTAR_bugs_all[df_MORTAR_bugs_all[\"dialogue_key\"].isin(all_403_dialogues.keys())]\n",
    "df_MORTAR_bugs[\"bug_round_seed\"] = df_MORTAR_bugs[\"dialogue_key\"] + \"##\" + df_MORTAR_bugs[\"original_round_num\"]\n",
    "df_METAL_bugs = pd.read_pickle(\"RQ1_FOUNDED_Bugs_METAL_with_bug_type.pickle\")\n",
    "df_METAL_bugs = df_METAL_bugs[df_METAL_bugs[\"dialogue_key\"].isin(all_403_dialogues.keys())]\n",
    "df_METAL_bugs[\"bug_round_seed\"] = df_METAL_bugs[\"dialogue_key\"] + \"##\" + df_METAL_bugs[\"original_round_num\"]\n",
    "\n",
    "bug_levels = [1,2,3]\n",
    "all_lms = [\"qwen2_0B5\", \"qwen2_1B5\", \"qwen2_7B\", \"mistral03_7B\", \"llama3_8B\", \"gemma2_9B\"]\n",
    "dist_LLM_DS_name = {\"qwen2_0B5\":\"DS1\", \"qwen2_1B5\":\"DS2\", \"qwen2_7B\":\"DS3\", \"mistral03_7B\":\"DS4\", \"llama3_8B\":\"DS5\", \"gemma2_9B\":\"DS6\"}\n",
    "\n",
    "\n",
    "# bug_level = bug_levels[0]\n",
    "\n",
    "\n",
    "venn_container_403 = {}\n",
    "\n",
    "MORTAR_unique_l1_bugs = []\n",
    "MORTAR_unique_l2_bugs = []\n",
    "MORTAR_unique_l3_bugs = []\n",
    "\n",
    "METAL_unique_l1_bugs_403 = []\n",
    "METAL_unique_l2_bugs_403 = []\n",
    "METAL_unique_l3_bugs_403 = []\n",
    "\n",
    "for llm in all_lms:\n",
    "    venn_container_403[llm] = {}\n",
    "    for bug_level in bug_levels:\n",
    "        venn_container_403[llm][bug_level] = {\"MORTAR\":[], \"METAL\":[], \"Overlap\":[]}\n",
    "\n",
    "        df_MORTAR_llm_this_buglevel = df_MORTAR_bugs[\n",
    "            (df_MORTAR_bugs[\"bug_type\"] == bug_level) &\n",
    "            (df_MORTAR_bugs[\"SUT\"] == llm)\n",
    "            ]\n",
    "        list_MORTAR_llm_this_buglevel_bug_round_seed = df_MORTAR_llm_this_buglevel[\"bug_round_seed\"].tolist()\n",
    "        venn_container_403[llm][bug_level][\"MORTAR\"] = list_MORTAR_llm_this_buglevel_bug_round_seed\n",
    "\n",
    "        df_METAL_llm_this_buglevel = df_METAL_bugs[\n",
    "            (df_METAL_bugs[\"bug_type\"] == bug_level) &\n",
    "            (df_METAL_bugs[\"SUT\"] == llm)\n",
    "            ]\n",
    "        list_METAL_llm_this_buglevel_bug_round_seed = df_METAL_llm_this_buglevel[\"bug_round_seed\"].tolist()\n",
    "        venn_container_403[llm][bug_level][\"METAL\"] = list_METAL_llm_this_buglevel_bug_round_seed\n",
    "\n",
    "        venn_container_403[llm][bug_level][\"Overlap\"] = [item for item in list_MORTAR_llm_this_buglevel_bug_round_seed if item in list_METAL_llm_this_buglevel_bug_round_seed]\n",
    "\n",
    "        this_llm_this_level_mortar_unique_bugs = [item for item in list_MORTAR_llm_this_buglevel_bug_round_seed if item not in list_METAL_llm_this_buglevel_bug_round_seed]\n",
    "        this_llm_this_level_metal_unique_bugs = [item for item in list_METAL_llm_this_buglevel_bug_round_seed if item not in list_MORTAR_llm_this_buglevel_bug_round_seed]\n",
    "\n",
    "        if bug_level == 1:\n",
    "            MORTAR_unique_l1_bugs += this_llm_this_level_mortar_unique_bugs\n",
    "            METAL_unique_l1_bugs_403 += this_llm_this_level_metal_unique_bugs\n",
    "        elif bug_level == 2:\n",
    "            MORTAR_unique_l2_bugs += this_llm_this_level_mortar_unique_bugs\n",
    "            METAL_unique_l2_bugs_403 += this_llm_this_level_metal_unique_bugs\n",
    "        elif bug_level == 3:\n",
    "            MORTAR_unique_l3_bugs += this_llm_this_level_mortar_unique_bugs\n",
    "            METAL_unique_l3_bugs_403 += this_llm_this_level_metal_unique_bugs\n",
    "\n",
    "# METAL\n",
    "print(\"METAL SUTs & L1-Bugs & L2-Bugs & L3-Bugs & Overall \\\\\\\\ \")\n",
    "print(\"\\\\midrule\")\n",
    "\n",
    "for llm in all_lms:\n",
    "    metal_l1_unique_bugs_403 = [item for item in venn_container_403[llm][1][\"METAL\"] if item not in venn_container_403[llm][1][\"Overlap\"]]\n",
    "    metal_l2_unique_bugs_403 = [item for item in venn_container_403[llm][2][\"METAL\"] if item not in venn_container_403[llm][2][\"Overlap\"]]\n",
    "    metal_l3_unique_bugs_403 = [item for item in venn_container_403[llm][3][\"METAL\"] if item not in venn_container_403[llm][3][\"Overlap\"]]\n",
    "    print(\"{} & {}\\% & {}\\% & {}\\% & {}\\% \\\\\\\\ \".format(\n",
    "        dist_LLM_DS_name[llm],\n",
    "        round(len(metal_l1_unique_bugs_403)/len(venn_container_403[llm][1][\"METAL\"])*100, 1),\n",
    "        round(len(metal_l2_unique_bugs_403)/len(venn_container_403[llm][2][\"METAL\"])*100, 1),\n",
    "        round(len(metal_l3_unique_bugs_403)/len(venn_container_403[llm][3][\"METAL\"])*100, 1) if len(venn_container_403[llm][3][\"METAL\"])!=0 else 0,\n",
    "        round((len(metal_l1_unique_bugs_403) + len(metal_l2_unique_bugs_403) + len(metal_l3_unique_bugs_403))/(len(venn_container_403[llm][1][\"METAL\"]) + len( venn_container_403[llm][2][\"METAL\"]) + len(venn_container_403[llm][3][\"METAL\"]))*100, 1)\n",
    "    ))\n",
    "\n",
    "print(\"\\\\midrule\")\n",
    "print(\"Average & {}\\% & {}\\% & {}\\% & {}\\% \\\\\\\\\".format(\n",
    "    round(len(METAL_unique_l1_bugs_403)/len(df_METAL_bugs[df_METAL_bugs[\"bug_type\"] == 1])*100,1),\n",
    "    round(len(METAL_unique_l2_bugs_403)/len(df_METAL_bugs[df_METAL_bugs[\"bug_type\"] == 2])*100,1),\n",
    "    round(len(METAL_unique_l3_bugs_403)/len(df_METAL_bugs[df_METAL_bugs[\"bug_type\"] == 3])*100,1),\n",
    "    round((len(METAL_unique_l1_bugs_403) + len(METAL_unique_l2_bugs_403) + len(METAL_unique_l3_bugs_403))/(len(df_METAL_bugs[df_METAL_bugs[\"bug_type\"] == 1]) + len(df_METAL_bugs[df_METAL_bugs[\"bug_type\"] == 2]) + len(df_METAL_bugs[df_METAL_bugs[\"bug_type\"] == 3]))*100,1)\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36256a71",
   "metadata": {},
   "source": [
    "# RQ3: Component Contribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f08a2e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import json \n",
    "import pandas as pd\n",
    "\n",
    "MORTAR_Perts = [\n",
    "    \"P1_round_shuffle\",\n",
    "    \"P2_round_reduce\",\n",
    "    \"P3_round_duplicate\",\n",
    "    \"P4_round_reduce_shuffle\",\n",
    "    \"P5_round_shuffle_duplicate\",\n",
    "    ]\n",
    "\n",
    "dataset_jsons = {}\n",
    "\n",
    "for this_pert in MORTAR_Perts:\n",
    "    with open(f\"data/{this_pert}.json\", \"r\") as f:\n",
    "        dataset_jsons[this_pert] = json.load(f)\n",
    "\n",
    "for this_pert in MORTAR_Perts:\n",
    "    id_pert_dialogues = []\n",
    "    id_pert_dialogue_has_uanswerability_round = []\n",
    "    id_pert_dialogue_usage_mr1 = []\n",
    "    id_pert_dialogue_usage_mr2 = []\n",
    "\n",
    "    id_pert_rounds = []\n",
    "    id_pert_rounds_un_answerable = []\n",
    "    id_pert_rounds_answerability_changed = []\n",
    "    \n",
    "    this_pert_dataset = dataset_jsons[this_pert]\n",
    "    for dialogue_key in this_pert_dataset.keys():\n",
    "        id_pert_dialogues.append(dialogue_key)\n",
    "        for pert_round_key in this_pert_dataset[dialogue_key].keys():\n",
    "            pert_round_uid = dialogue_key + \"##\" + pert_round_key\n",
    "            id_pert_rounds.append(pert_round_uid)\n",
    "\n",
    "            if this_pert_dataset[dialogue_key][pert_round_key][\"FLAG_answer_changed\"]:\n",
    "                id_pert_rounds_answerability_changed.append(pert_round_uid)\n",
    "                id_pert_dialogue_usage_mr2.append(dialogue_key) if dialogue_key not in id_pert_dialogue_usage_mr2 else None\n",
    "\n",
    "            if this_pert_dataset[dialogue_key][pert_round_key][\"Answer\"] == \"unknown\":\n",
    "                id_pert_rounds_un_answerable.append(pert_round_uid)\n",
    "                id_pert_dialogue_has_uanswerability_round.append(dialogue_key) if dialogue_key not in id_pert_dialogue_has_uanswerability_round else None\n",
    "            else:\n",
    "                id_pert_dialogue_usage_mr1.append(dialogue_key) if dialogue_key not in id_pert_dialogue_usage_mr1 else None\n",
    "            \n",
    "mapping_original_round_appear_in_pert = {} # Oid-r1 -> [P1-r1, P2-r3 ...]\n",
    "all_original_round_ids = []\n",
    "\n",
    "dialogue_round_keys_403 = json.load(open(\"data/dialogue_round_keys_403.json\", \"r\"))\n",
    "for original_dialogue_key in dialogue_round_keys_403.keys():\n",
    "    for original_round_key in dialogue_round_keys_403[original_dialogue_key]:\n",
    "        original_round_uid = original_dialogue_key + \"##\" + original_round_key\n",
    "        all_original_round_ids.append(original_round_uid)\n",
    "        mapping_original_round_appear_in_pert[original_round_uid] = []\n",
    "\n",
    "for pert_type in MORTAR_Perts:\n",
    "    this_pert_dataset = dataset_jsons[pert_type]\n",
    "    for pert_dialogue_key in this_pert_dataset.keys():\n",
    "        for pert_round_key in this_pert_dataset[pert_dialogue_key].keys():\n",
    "            pert_round_uid = pert_type + \"#pert#\" +pert_dialogue_key + \"#pert#\" + pert_round_key\n",
    "            original_round = this_pert_dataset[pert_dialogue_key][pert_round_key]['is_original_round']\n",
    "            original_round_uid = pert_dialogue_key + \"##\" + original_round # key in mapping_original_round_appear_in_pert\n",
    "            mapping_original_round_appear_in_pert[original_round_uid].append(pert_round_uid)\n",
    "\n",
    "\n",
    "list_pert_dialogue_usage_mr1 = {} # O1-r3 : [P1-O1-r2, P2...], (pert_round_uid), expect output(P1-O1-r2) == answer(O1-r3)\n",
    "list_pert_dialogue_usage_mr2 = {} # O1-r2 : [P2-O1-r1, ...], (pert_round_uid), expect output(P2-O1-r1) != answer(O1-r2)\n",
    "\n",
    "dict_pert_dialogue_usage_mr3 = {} # O1-r1 : {1:[P1-O1-r1, P2-O1-r3, ...]}, expect output(P1-O1-r1) == output(P2-O1-r3) and ...\n",
    "dict_pert_dialogue_usage_mr4 = {} # O1-r1 : {0:[P2-O1-r1] 1:[P1-O1-r1, P2-O1-r3]}, expect output(P2-O1-r1) != output(P1-O1-r1) and output(P2-O1-r1) != output(P2-O1-r3)\n",
    "\n",
    "for original_round_id in all_original_round_ids:\n",
    "    list_pert_dialogue_usage_mr1[original_round_id] = []\n",
    "    list_pert_dialogue_usage_mr2[original_round_id] = []\n",
    "\n",
    "    this_ori_dialogue_appear_in_pert = mapping_original_round_appear_in_pert[original_round_id]\n",
    "\n",
    "    pert_answerable_q_sum = 0\n",
    "    dict_pert_answerability = {}\n",
    "\n",
    "    for pert_round_id in this_ori_dialogue_appear_in_pert:\n",
    "        pert_type = pert_round_id.split(\"#pert#\")[0]\n",
    "        pert_dialogue_key = pert_round_id.split(\"#pert#\")[1]\n",
    "        pert_round_key = pert_round_id.split(\"#pert#\")[2]\n",
    "\n",
    "        answerability_this_pert_round = (dataset_jsons[pert_type][pert_dialogue_key][pert_round_key][\"Answer\"] != \"unknown\") # changed answerability\n",
    "        dict_pert_answerability[pert_round_key] = answerability_this_pert_round\n",
    "        pert_answerable_q_sum += answerability_this_pert_round\n",
    "\n",
    "        if answerability_this_pert_round:\n",
    "            list_pert_dialogue_usage_mr1[original_round_id].append(pert_round_id)\n",
    "        else:\n",
    "            if dataset_jsons[pert_type][pert_dialogue_key][pert_round_key][\"FLAG_answer_changed\"]:\n",
    "                list_pert_dialogue_usage_mr2[original_round_id].append(pert_round_id)\n",
    "\n",
    "    if pert_answerable_q_sum == len(this_ori_dialogue_appear_in_pert) and pert_answerable_q_sum > 0: # all pert rounds are answerable\n",
    "        dict_pert_dialogue_usage_mr3[original_round_id] = {1:[]}\n",
    "        dict_pert_dialogue_usage_mr3[original_round_id][1] = copy.deepcopy(list_pert_dialogue_usage_mr1[original_round_id])\n",
    "    elif 0<pert_answerable_q_sum<len(this_ori_dialogue_appear_in_pert): # some pert rounds are answerable, some are not\n",
    "        dict_pert_dialogue_usage_mr4[original_round_id] = {0:[],1:[]}\n",
    "        dict_pert_dialogue_usage_mr4[original_round_id][0] = copy.deepcopy(list_pert_dialogue_usage_mr2[original_round_id])\n",
    "        dict_pert_dialogue_usage_mr4[original_round_id][1] = copy.deepcopy(list_pert_dialogue_usage_mr1[original_round_id])\n",
    "\n",
    "for original_round_id in list_pert_dialogue_usage_mr1.copy().keys():\n",
    "    if len(list_pert_dialogue_usage_mr1[original_round_id]) == 0:\n",
    "        del list_pert_dialogue_usage_mr1[original_round_id]\n",
    "\n",
    "for original_round_id in list_pert_dialogue_usage_mr2.copy().keys():\n",
    "    if len(list_pert_dialogue_usage_mr2[original_round_id]) == 0:\n",
    "        del list_pert_dialogue_usage_mr2[original_round_id]\n",
    "\n",
    "for original_round_id in dict_pert_dialogue_usage_mr3.copy().keys():\n",
    "    if len(dict_pert_dialogue_usage_mr3[original_round_id][1]) < 2:\n",
    "        del dict_pert_dialogue_usage_mr3[original_round_id]\n",
    "\n",
    "for original_round_id in dict_pert_dialogue_usage_mr4.copy().keys():\n",
    "    if len(dict_pert_dialogue_usage_mr4[original_round_id][0]) == 0 or len(dict_pert_dialogue_usage_mr4[original_round_id][1]) == 0:\n",
    "        del dict_pert_dialogue_usage_mr4[original_round_id]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0de62f7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30385\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'P1_round_shuffle': 6315,\n",
       " 'P2_round_reduce': 4539,\n",
       " 'P3_round_duplicate': 7548,\n",
       " 'P4_round_reduce_shuffle': 4510,\n",
       " 'P5_round_shuffle_duplicate': 7473}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "usage_mr1 = {\n",
    "    \"P1_round_shuffle\":0,\n",
    "    \"P2_round_reduce\":0,\n",
    "    \"P3_round_duplicate\":0,\n",
    "    \"P4_round_reduce_shuffle\":0,\n",
    "    \"P5_round_shuffle_duplicate\":0,\n",
    "}\n",
    "\n",
    "for original_round_id in list_pert_dialogue_usage_mr1.keys():\n",
    "    pert_round_ids = list_pert_dialogue_usage_mr1[original_round_id]\n",
    "    for pert_round_id in pert_round_ids:\n",
    "        pert_type = pert_round_id.split(\"#pert#\")[0]\n",
    "        usage_mr1[pert_type] += 1\n",
    "print(usage_mr1[\"P1_round_shuffle\"]+ usage_mr1[\"P2_round_reduce\"]+ usage_mr1[\"P3_round_duplicate\"]+ usage_mr1[\"P4_round_reduce_shuffle\"]+ usage_mr1[\"P5_round_shuffle_duplicate\"])\n",
    "usage_mr1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "192258c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "178\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'P1_round_shuffle': 60,\n",
       " 'P2_round_reduce': 24,\n",
       " 'P3_round_duplicate': 0,\n",
       " 'P4_round_reduce_shuffle': 53,\n",
       " 'P5_round_shuffle_duplicate': 41}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "usage_mr2 = {\n",
    "    \"P1_round_shuffle\":0,\n",
    "    \"P2_round_reduce\":0,\n",
    "    \"P3_round_duplicate\":0,\n",
    "    \"P4_round_reduce_shuffle\":0,\n",
    "    \"P5_round_shuffle_duplicate\":0,\n",
    "}\n",
    "\n",
    "for original_round_id in list_pert_dialogue_usage_mr2.keys():\n",
    "    pert_round_ids = list_pert_dialogue_usage_mr2[original_round_id]\n",
    "    for pert_round_id in pert_round_ids:\n",
    "        pert_type = pert_round_id.split(\"#pert#\")[0]\n",
    "        usage_mr2[pert_type] += 1\n",
    "\n",
    "print(usage_mr2[\"P1_round_shuffle\"]+usage_mr2[\"P2_round_reduce\"]+usage_mr2[\"P3_round_duplicate\"]+usage_mr2[\"P4_round_reduce_shuffle\"]+usage_mr2[\"P5_round_shuffle_duplicate\"])\n",
    "usage_mr2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0059b00c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6194\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'P1_round_shuffle': 6194,\n",
       " 'P2_round_reduce': 4426,\n",
       " 'P3_round_duplicate': 7347,\n",
       " 'P4_round_reduce_shuffle': 4431,\n",
       " 'P5_round_shuffle_duplicate': 7341}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "usage_mr3 = {\n",
    "    \"P1_round_shuffle\":0,\n",
    "    \"P2_round_reduce\":0,\n",
    "    \"P3_round_duplicate\":0,\n",
    "    \"P4_round_reduce_shuffle\":0,\n",
    "    \"P5_round_shuffle_duplicate\":0,\n",
    "}\n",
    "\n",
    "for original_round_id in dict_pert_dialogue_usage_mr3.keys():\n",
    "    true_pert_round_ids = dict_pert_dialogue_usage_mr3[original_round_id][1]\n",
    "    for pert_round_id in true_pert_round_ids:\n",
    "        pert_type = pert_round_id.split(\"#pert#\")[0]\n",
    "        usage_mr3[pert_type] += 1\n",
    "print(len(dict_pert_dialogue_usage_mr3))\n",
    "usage_mr3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d1e016f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "151\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'P1_round_shuffle': 151,\n",
       " 'P2_round_reduce': 114,\n",
       " 'P3_round_duplicate': 177,\n",
       " 'P4_round_reduce_shuffle': 116,\n",
       " 'P5_round_shuffle_duplicate': 164}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "usage_mr4 = {\n",
    "    \"P1_round_shuffle\":0,\n",
    "    \"P2_round_reduce\":0,\n",
    "    \"P3_round_duplicate\":0,\n",
    "    \"P4_round_reduce_shuffle\":0,\n",
    "    \"P5_round_shuffle_duplicate\":0,\n",
    "}\n",
    "\n",
    "for original_round_id in dict_pert_dialogue_usage_mr4.keys():\n",
    "    true_pert_round_ids = dict_pert_dialogue_usage_mr4[original_round_id][1]\n",
    "    false_pert_round_ids = dict_pert_dialogue_usage_mr4[original_round_id][0]\n",
    "\n",
    "    for pert_round_id in true_pert_round_ids:\n",
    "        pert_type = pert_round_id.split(\"#pert#\")[0]\n",
    "        usage_mr4[pert_type] += 1\n",
    "    for pert_round_id in false_pert_round_ids:\n",
    "        pert_type = pert_round_id.split(\"#pert#\")[0]\n",
    "        usage_mr4[pert_type] += 1\n",
    "\n",
    "print(len(dict_pert_dialogue_usage_mr4))\n",
    "usage_mr4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ac1823",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DS1 &11380 (35.6\\%) &7110 (22.3\\%) &8494 (26.6\\%) &4155 (13.0\\%) &6835 (21.4\\%) \\\\\n",
      "DS2 &10369 (37.3\\%) &6296 (22.6\\%) &7584 (27.3\\%) &3604 (13.0\\%) &5878 (21.1\\%) \\\\\n",
      "DS3 &8433 (34.2\\%) &4864 (19.7\\%) &6414 (26.0\\%) &3511 (14.3\\%) &5799 (23.5\\%) \\\\\n",
      "DS4 &7479 (32.6\\%) &4499 (19.6\\%) &6003 (26.2\\%) &3448 (15.0\\%) &5322 (23.2\\%) \\\\\n",
      "DS5 &6775 (39.2\\%) &3807 (22.1\\%) &4262 (24.7\\%) &2599 (15.1\\%) &3920 (22.7\\%) \\\\\n",
      "DS6 &5200 (40.6\\%) &2623 (20.5\\%) &3042 (23.8\\%) &1856 (14.5\\%) &3160 (24.7\\%) \\\\\n",
      "\\midrule\n",
      "Average & 8273(36.1\\%) & 4866(21.3\\%) & 5966(26.1\\%) & 3196(14.0\\%) & 5152(22.5\\%) \\\\\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df_MORTAR_bugs = pd.read_pickle(\"RQ1_FOUNDED_Bugs_MORTAR_with_bug_type.pickle\")\n",
    "\n",
    "all_lms = [\"qwen2_0B5\", \"qwen2_1B5\", \"qwen2_7B\", \"mistral03_7B\", \"llama3_8B\", \"gemma2_9B\"]\n",
    "MORTAR_Perts = [\"P1_round_shuffle\",\"P2_round_reduce\",\"P3_round_duplicate\",\"P4_round_reduce_shuffle\",\"P5_round_shuffle_duplicate\"]\n",
    "dist_LLM_DS_name = {\"qwen2_0B5\":\"DS1\", \"qwen2_1B5\":\"DS2\", \"qwen2_7B\":\"DS3\", \"mistral03_7B\":\"DS4\", \"llama3_8B\":\"DS5\", \"gemma2_9B\":\"DS6\"}\n",
    "\n",
    "result_rq_321 = {\n",
    "    \"qwen2_0B5\": {},\n",
    "    \"qwen2_1B5\": {},\n",
    "    \"qwen2_7B\": {},\n",
    "    \"mistral03_7B\": {},\n",
    "    \"llama3_8B\": {},\n",
    "    \"gemma2_9B\": {}\n",
    "}\n",
    "\n",
    "for lm in all_lms:\n",
    "    result_rq_321[lm] = {\n",
    "        \"P1_round_shuffle\": 0,\n",
    "        \"P2_round_reduce\": 0,\n",
    "        \"P3_round_duplicate\": 0,\n",
    "        \"P4_round_reduce_shuffle\": 0,\n",
    "        \"P5_round_shuffle_duplicate\": 0,\n",
    "    }\n",
    "    for pert in MORTAR_Perts:\n",
    "        result_rq_321[lm][pert] += len(df_MORTAR_bugs[(df_MORTAR_bugs[\"use_pert_1\"] == pert) & (df_MORTAR_bugs[\"SUT\"] == lm)])\n",
    "        result_rq_321[lm][pert] += len(df_MORTAR_bugs[(df_MORTAR_bugs[\"use_pert_2\"] == pert) & (df_MORTAR_bugs[\"SUT\"] == lm)])\n",
    "    \n",
    "    print(\"{} &{} ({}\\%) &{} ({}\\%) &{} ({}\\%) &{} ({}\\%) &{} ({}\\%) \\\\\\\\\".format(\n",
    "        dist_LLM_DS_name[lm],\n",
    "        result_rq_321[lm][\"P1_round_shuffle\"],\n",
    "        round(result_rq_321[lm][\"P1_round_shuffle\"]/len(df_MORTAR_bugs[df_MORTAR_bugs[\"SUT\"] == lm])*100,1),\n",
    "        result_rq_321[lm][\"P2_round_reduce\"],\n",
    "        round(result_rq_321[lm][\"P2_round_reduce\"]/len(df_MORTAR_bugs[df_MORTAR_bugs[\"SUT\"] == lm])*100,1),\n",
    "        result_rq_321[lm][\"P3_round_duplicate\"],\n",
    "        round(result_rq_321[lm][\"P3_round_duplicate\"]/len(df_MORTAR_bugs[df_MORTAR_bugs[\"SUT\"] == lm])*100,1),\n",
    "        result_rq_321[lm][\"P4_round_reduce_shuffle\"],\n",
    "        round(result_rq_321[lm][\"P4_round_reduce_shuffle\"]/len(df_MORTAR_bugs[df_MORTAR_bugs[\"SUT\"] == lm])*100,1),\n",
    "        result_rq_321[lm][\"P5_round_shuffle_duplicate\"],\n",
    "        round(result_rq_321[lm][\"P5_round_shuffle_duplicate\"]/len(df_MORTAR_bugs[df_MORTAR_bugs[\"SUT\"] == lm])*100,1),)\n",
    "    )\n",
    "\n",
    "print(\"\\midrule\")\n",
    "print(\"Average & {}({}\\%) & {}({}\\%) & {}({}\\%) & {}({}\\%) & {}({}\\%) \\\\\\\\\".format(\n",
    "    \n",
    "    round(sum([result_rq_321[lm][\"P1_round_shuffle\"] for lm in all_lms])/6),\n",
    "    round(sum([result_rq_321[lm][\"P1_round_shuffle\"] for lm in all_lms])/len(df_MORTAR_bugs)*100,1),\n",
    "    round(sum([result_rq_321[lm][\"P2_round_reduce\"] for lm in all_lms])/6),\n",
    "    round(sum([result_rq_321[lm][\"P2_round_reduce\"] for lm in all_lms])/len(df_MORTAR_bugs)*100,1),\n",
    "    round(sum([result_rq_321[lm][\"P3_round_duplicate\"] for lm in all_lms])/6),\n",
    "    round(sum([result_rq_321[lm][\"P3_round_duplicate\"] for lm in all_lms])/len(df_MORTAR_bugs)*100,1),\n",
    "    round(sum([result_rq_321[lm][\"P4_round_reduce_shuffle\"] for lm in all_lms])/6),\n",
    "    round(sum([result_rq_321[lm][\"P4_round_reduce_shuffle\"] for lm in all_lms])/len(df_MORTAR_bugs)*100,1),\n",
    "    round(sum([result_rq_321[lm][\"P5_round_shuffle_duplicate\"] for lm in all_lms])/6),\n",
    "    round(sum([result_rq_321[lm][\"P5_round_shuffle_duplicate\"] for lm in all_lms])/len(df_MORTAR_bugs)*100,1),)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "42149b35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DS1 &25903(81.1\\%) &7(0.02\\%) &5966(18.7\\%) &66(0.2\\%) \\\\\n",
      "DS2 &21861(78.6\\%) &6(0.02\\%) &5844(21.0\\%) &88(0.3\\%) \\\\\n",
      "DS3 &20242(82.2\\%) &5(0.02\\%) &4275(17.4\\%) &112(0.5\\%) \\\\\n",
      "DS4 &19074(83.2\\%) &13(0.06\\%) &3692(16.1\\%) &140(0.6\\%) \\\\\n",
      "DS5 &13153(76.2\\%) &12(0.07\\%) &3975(23.0\\%) &124(0.7\\%) \\\\\n",
      "DS6 &9722(75.9\\%) &7(0.05\\%) &2956(23.1\\%) &120(0.9\\%) \\\\\n",
      "\\midrule\n",
      "Average & 18326(80.0\\%) & 8(0.04\\%) & 4451(19.4\\%) & 108(0.5\\%) \\\\\n"
     ]
    }
   ],
   "source": [
    "# MR contribution to bug discovery\n",
    "\n",
    "all_lms = [\"qwen2_0B5\", \"qwen2_1B5\", \"qwen2_7B\", \"mistral03_7B\", \"llama3_8B\", \"gemma2_9B\"]\n",
    "MRs = [\"MR1\", \"MR2\", \"MR3\", \"MR4\"]\n",
    "dist_LLM_DS_name = {\"qwen2_0B5\":\"DS1\", \"qwen2_1B5\":\"DS2\", \"qwen2_7B\":\"DS3\", \"mistral03_7B\":\"DS4\", \"llama3_8B\":\"DS5\", \"gemma2_9B\":\"DS6\"}\n",
    "\n",
    "result_rq_322 = {\n",
    "    \"qwen2_0B5\": {},\n",
    "    \"qwen2_1B5\": {},\n",
    "    \"qwen2_7B\": {},\n",
    "    \"mistral03_7B\": {},\n",
    "    \"llama3_8B\": {},\n",
    "    \"gemma2_9B\": {}\n",
    "}\n",
    "\n",
    "for lm in all_lms:\n",
    "    result_rq_322[lm] = {\n",
    "        \"MR1\": 0,\n",
    "        \"MR2\": 0,\n",
    "        \"MR3\": 0,\n",
    "        \"MR4\": 0,\n",
    "    }\n",
    "    for MR in MRs:\n",
    "        result_rq_322[lm][MR] += len(df_MORTAR_bugs[(df_MORTAR_bugs[\"use_mr\"] == MR) & (df_MORTAR_bugs[\"SUT\"] == lm)])\n",
    "\n",
    "    print(\"{} &{}({}\\%) &{}({}\\%) &{}({}\\%) &{}({}\\%) \\\\\\\\\".format(\n",
    "        dist_LLM_DS_name[lm],\n",
    "        result_rq_322[lm][\"MR1\"],\n",
    "        round(result_rq_322[lm][\"MR1\"]/len(df_MORTAR_bugs[df_MORTAR_bugs[\"SUT\"] == lm])*100,1),\n",
    "        result_rq_322[lm][\"MR2\"],\n",
    "        round(result_rq_322[lm][\"MR2\"]/len(df_MORTAR_bugs[df_MORTAR_bugs[\"SUT\"] == lm])*100,2),\n",
    "        result_rq_322[lm][\"MR3\"],\n",
    "        round(result_rq_322[lm][\"MR3\"]/len(df_MORTAR_bugs[df_MORTAR_bugs[\"SUT\"] == lm])*100,1),\n",
    "        result_rq_322[lm][\"MR4\"],\n",
    "        round(result_rq_322[lm][\"MR4\"]/len(df_MORTAR_bugs[df_MORTAR_bugs[\"SUT\"] == lm])*100,1),\n",
    "    ))\n",
    "print(\"\\midrule\")\n",
    "\n",
    "print(\"Average & {}({}\\%) & {}({}\\%) & {}({}\\%) & {}({}\\%) \\\\\\\\\".format(\n",
    "    round(sum([result_rq_322[lm][\"MR1\"] for lm in all_lms])/6),\n",
    "    round(sum([result_rq_322[lm][\"MR1\"] for lm in all_lms])/len(df_MORTAR_bugs)*100,1),\n",
    "    round(sum([result_rq_322[lm][\"MR2\"] for lm in all_lms])/6),\n",
    "    round(sum([result_rq_322[lm][\"MR2\"] for lm in all_lms])/len(df_MORTAR_bugs)*100,2),\n",
    "\n",
    "    round(sum([result_rq_322[lm][\"MR3\"] for lm in all_lms])/6),\n",
    "    round(sum([result_rq_322[lm][\"MR3\"] for lm in all_lms])/len(df_MORTAR_bugs)*100,1),\n",
    "\n",
    "    round(sum([result_rq_322[lm][\"MR4\"] for lm in all_lms])/6),\n",
    "    round(sum([result_rq_322[lm][\"MR4\"] for lm in all_lms])/len(df_MORTAR_bugs)*100,1),\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bf242d2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MR1 & 78.2\\% & 21.4\\% & 0.4\\% \\\\ \n",
      "MR2 & 48.0\\% & 52.0\\% & 0.0\\% \\\\ \n",
      "MR3 & 66.9\\% & 32.4\\% & 0.7\\% \\\\ \n",
      "MR4 & 51.4\\% & 47.5\\% & 1.1\\% \\\\ \n"
     ]
    }
   ],
   "source": [
    "for MR in MRs:\n",
    "    print(\"{} & {}\\% & {}\\% & {}\\% \\\\\\\\ \".format(\n",
    "        MR,\n",
    "        \n",
    "        round(len(df_MORTAR_bugs[(df_MORTAR_bugs[\"bug_type\"] == 1) & (df_MORTAR_bugs[\"use_mr\"] == MR)])/len(df_MORTAR_bugs[df_MORTAR_bugs[\"use_mr\"] == MR])*100,1),\n",
    "        \n",
    "        round(len(df_MORTAR_bugs[(df_MORTAR_bugs[\"bug_type\"] == 2) & (df_MORTAR_bugs[\"use_mr\"] == MR)])/len(df_MORTAR_bugs[df_MORTAR_bugs[\"use_mr\"] == MR])*100,1),\n",
    "        \n",
    "        round(len(df_MORTAR_bugs[(df_MORTAR_bugs[\"bug_type\"] == 3) & (df_MORTAR_bugs[\"use_mr\"] == MR)])/len(df_MORTAR_bugs[df_MORTAR_bugs[\"use_mr\"] == MR])*100,1),\n",
    "    ))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
