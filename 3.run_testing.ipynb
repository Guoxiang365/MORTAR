{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b275584e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "import json\n",
    "import datetime\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "import argparse\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from multi_turn_test import llm_pipeline\n",
    "from multi_turn_test import MultiTurnConversation as mtc\n",
    "\n",
    "\n",
    "choose_lm = 4 #gemma2_9B\n",
    "\n",
    "# make dialogue wrapper\n",
    "all_lms = [\"qwen2_0B5\", \"qwen2_1B5\", \"qwen2_7B\", \"llama3_8B\", \"gemma2_9B\", \"mistral03_7B\"]\n",
    "test_lm = all_lms[choose_lm]\n",
    "\n",
    "LLM_DS = {\n",
    "    \"gemma2_9B\": llm_pipeline.gemma_2_9B_pipeline(),\n",
    "}\n",
    "obj_llm = LLM_DS[test_lm]\n",
    "pipe, pipe_config, = obj_llm.pipe_init()\n",
    "\n",
    "# make path\n",
    "result_folder = \"result/MTMT/\"+test_lm\n",
    "os.makedirs(result_folder,exist_ok=True)\n",
    "result_path = result_folder+\"/{}.{}\"\n",
    "\n",
    "# start running\n",
    "MRs = [\n",
    "    \"P0_round_original\",\n",
    "    \"P1_round_shuffle\",\n",
    "    \"P2_round_reduce\",\n",
    "    \"P3_round_duplicate\",\n",
    "    \"P4_round_reduce_shuffle\",\n",
    "    \"P5_round_shuffle_duplicate\",\n",
    "    \"MR10_SNP_s06_synonym_replacement\",\n",
    "    \"MR11_SNP_s06_introduce_typos\",\n",
    "    \"MR12_SNP_s06_to_leet\",\n",
    "    \"MR13_SNP_s06_add_words\",\n",
    "    ]\n",
    "\n",
    "for dataset_name in MRs:\n",
    "    print(test_lm, dataset_name)\n",
    "    path_dataset = \"data/{}.json\".format(dataset_name)\n",
    "\n",
    "    def current_time_string():\n",
    "        return datetime.datetime.now().strftime('%Y%m%d-%H%M%S')\n",
    "\n",
    "    print(current_time_string(),\"Start!\")\n",
    "\n",
    "    df_result = pd.DataFrame()\n",
    "\n",
    "    dataset_MTMT = json.load(open(path_dataset))\n",
    "    dataset_dev_full= json.load(open(\"data/extracted_dev_all_final_update_3.json\"))\n",
    "    all_keys = list(dataset_MTMT.keys())[:]\n",
    "    template_system_info = \"You are a smart reader. Please read the following story:\\n{}\\nI'm going to ask you some questions about the story. Please give me concise, exact, and short answer. If you are not sure about what I am asking, or any information in question is ambiguous, answer me 'unknown'. \\nHere comes the first question: {}\"\n",
    "\n",
    "    for this_dialogue_key in tqdm(all_keys):\n",
    "        this_dialogue = dataset_MTMT[this_dialogue_key]\n",
    "        this_dialogue_round_keys = list(this_dialogue.keys())\n",
    "\n",
    "        story_id = this_dialogue_key\n",
    "        story_content = dataset_dev_full[story_id][\"story_material\"]\n",
    "\n",
    "        # start conversation!\n",
    "        this_conv = mtc.multi_turn_conversation(f_llm_pipeline=pipe, pipeline_params=pipe_config)\n",
    "       \n",
    "        for round_i, round_key in enumerate(this_dialogue_round_keys):\n",
    "\n",
    "            thisround_question = this_dialogue[round_key][\"Question\"]\n",
    "            thisround_goldanswer = this_dialogue[round_key][\"Answer\"]\n",
    "            round_note = this_dialogue[round_key]\n",
    "            if round_i == 0: \n",
    "                this_conv.new_rounds(message_body=template_system_info.format(story_content, thisround_question), expected_answer=[thisround_goldanswer], source_uuid=story_id, note=round_note)\n",
    "            else:\n",
    "                this_conv.new_rounds(message_body=thisround_question, expected_answer=[thisround_goldanswer], source_uuid=story_id, note=round_note)\n",
    "                \n",
    "        df_result=pd.concat([df_result,this_conv.end_chat(evaluate=True)]).reset_index(drop=True)\n",
    "\n",
    "    df_result.to_pickle(result_path.format(dataset_name,\"pickle\"))\n",
    "    df_result.to_csv(result_path.format(dataset_name,\"csv\"))\n",
    "    print(current_time_string(),\"End\")\n",
    "\n",
    "del pipe, pipe_config, obj_llm\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f2b7275",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "import json\n",
    "import datetime\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "import argparse\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from multi_turn_test import llm_pipeline\n",
    "from multi_turn_test import MultiTurnConversation as mtc\n",
    "\n",
    "# make dialogue wrapper\n",
    "all_lms = [\"qwen2_0B5\", \"qwen2_1B5\", \"qwen2_7B\", \"llama3_8B\", \"mistral03_7B\"]\n",
    "\n",
    "MRs = [\n",
    "    \"P0_round_original\",\n",
    "    \"P1_round_shuffle\",\n",
    "    \"P2_round_reduce\",\n",
    "    \"P3_round_duplicate\",\n",
    "    \"P4_round_reduce_shuffle\",\n",
    "    \"P5_round_shuffle_duplicate\",\n",
    "    \"MR10_SNP_s06_synonym_replacement\",\n",
    "    \"MR11_SNP_s06_introduce_typos\",\n",
    "    \"MR12_SNP_s06_to_leet\",\n",
    "    \"MR13_SNP_s06_add_words\",\n",
    "    ]\n",
    "\n",
    "for choose_lm in range(len(all_lms)):\n",
    "    test_lm = all_lms[choose_lm]\n",
    "\n",
    "    LLM_DS = {\n",
    "        \"qwen2_0B5\": llm_pipeline.qwen_2_0B5_pipeline(),\n",
    "        \"qwen2_1B5\": llm_pipeline.qwen_2_1B5_pipeline(),\n",
    "        \"qwen2_7B\": llm_pipeline.qwen_2_7B_pipeline(),\n",
    "        \"llama3_8B\": llm_pipeline.llama_3_8B_pipeline(),\n",
    "        \"gemma2_9B\": llm_pipeline.gemma_2_9B_pipeline(),\n",
    "        \"mistral03_7B\": llm_pipeline.mistral03_7B_pipeline(),\n",
    "    }\n",
    "    obj_llm = LLM_DS[test_lm]\n",
    "    pipe, pipe_config, = obj_llm.pipe_init()\n",
    "\n",
    "    # make path\n",
    "    result_folder = \"result/MTMT/\"+test_lm\n",
    "    os.makedirs(result_folder,exist_ok=True)\n",
    "    result_path = result_folder+\"/{}.{}\"\n",
    "\n",
    "    for dataset_name in MRs:\n",
    "        print(test_lm, dataset_name)\n",
    "        path_dataset = \"data/{}.json\".format(dataset_name)\n",
    "\n",
    "        def current_time_string():\n",
    "            return datetime.datetime.now().strftime('%Y%m%d-%H%M%S')\n",
    "\n",
    "        print(current_time_string(),\"Start!\")\n",
    "\n",
    "        df_result = pd.DataFrame()\n",
    "\n",
    "        dataset_MTMT = json.load(open(path_dataset))\n",
    "        dataset_dev_full= json.load(open(\"data/extracted_dev_all_final_update_3.json\"))\n",
    "        all_keys = list(dataset_MTMT.keys())[:]\n",
    "        template_system_info = \"You are a smart reader. Please read the following story:\\n{}\\nI'm going to ask you some questions about the story. Please give me concise, exact, and short answer. If you are not sure about what I am asking, or any information in question is ambiguous, answer me 'unknown'.\"\n",
    "\n",
    "        for this_dialogue_key in tqdm(all_keys):\n",
    "            this_dialogue = dataset_MTMT[this_dialogue_key]\n",
    "            this_dialogue_round_keys = list(this_dialogue.keys())\n",
    "\n",
    "            story_id = this_dialogue_key\n",
    "            story_content = dataset_dev_full[story_id][\"story_material\"]\n",
    "\n",
    "            # start conversation!\n",
    "            this_conv = mtc.multi_turn_conversation(f_llm_pipeline=pipe, pipeline_params=pipe_config)\n",
    "            this_conv.new_rounds(message_body=template_system_info.format(story_content), role=\"system\", source_uuid=story_id, send_to_lm=False)\n",
    "\n",
    "            for round_key in this_dialogue_round_keys:\n",
    "                thisround_question = this_dialogue[round_key][\"Question\"]\n",
    "                thisround_goldanswer = this_dialogue[round_key][\"Answer\"]\n",
    "                round_note = this_dialogue[round_key]\n",
    "                returned_msg = this_conv.new_rounds(message_body=thisround_question, expected_answer=[thisround_goldanswer], source_uuid=story_id, note=round_note)\n",
    "                # print(\"Q:{}\\nExpect A:{}\\nLLM A:{}\\n\".format(thisround_question, thisround_goldanswer, returned_msg))\n",
    "\n",
    "            df_result=pd.concat([df_result,this_conv.end_chat(evaluate=True)]).reset_index(drop=True)\n",
    "\n",
    "        df_result.to_pickle(result_path.format(dataset_name,\"pickle\"))\n",
    "        df_result.to_csv(result_path.format(dataset_name,\"csv\"))\n",
    "        print(current_time_string(),\"End\")\n",
    "\n",
    "    del pipe, pipe_config, obj_llm\n",
    "    torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
